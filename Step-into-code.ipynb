{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_probs tensor([[[-2.9410, -1.0552, -1.4880, -2.8686, -3.5114, -1.8952, -1.9928]],\n",
      "\n",
      "        [[-3.5908, -1.0294, -1.5933, -2.5154, -1.3250, -4.8399, -2.8574]],\n",
      "\n",
      "        [[-1.7851, -0.9557, -1.9636, -2.7271, -2.1659, -2.7364, -2.7731]],\n",
      "\n",
      "        [[-2.3113, -1.0865, -4.4163, -1.4702, -1.6158, -3.1542, -2.5245]],\n",
      "\n",
      "        [[-2.3513, -1.2391, -6.1401, -1.8622, -1.9065, -1.2204, -4.2757]],\n",
      "\n",
      "        [[-2.0026, -1.0710, -1.8561, -1.8615, -1.8153, -3.3275, -4.4274]]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<LogSoftmaxBackward>)\n",
      "Device: cuda\n",
      "Log-probs shape (time X batch X channels): 6x1x7\n",
      "Built-in CTC loss fwd 0.00028228759765625 bwd 0.0007870197296142578\n",
      "B, tensor([0], device='cuda:0')\n",
      "targets, tensor([[1, 2, 3]], device='cuda:0')\n",
      "targets_, tensor([[1, 2, 3, 1]], device='cuda:0')\n",
      "targets_, tensor([[0, 1, 0, 2, 0, 3, 0, 1]], device='cuda:0')\n",
      "targets_[:, 2:] != targets_[:, :-2], tensor([[False,  True, False,  True, False,  True]], device='cuda:0')\n",
      "targets_[:, 2:], tensor([[0, 2, 0, 3, 0, 1]], device='cuda:0')\n",
      "targets_[:, :-2], tensor([[0, 1, 0, 2, 0, 3]], device='cuda:0')\n",
      "diff_labels, tensor([[False, False, False,  True, False,  True, False,  True]],\n",
      "       device='cuda:0')\n",
      "zero, -1.7976931348623157e+308\n",
      "log_probs, tensor([[[-2.9410, -1.0552, -1.4880, -2.8686, -3.5114, -1.8952, -1.9928]],\n",
      "\n",
      "        [[-3.5908, -1.0294, -1.5933, -2.5154, -1.3250, -4.8399, -2.8574]],\n",
      "\n",
      "        [[-1.7851, -0.9557, -1.9636, -2.7271, -2.1659, -2.7364, -2.7731]],\n",
      "\n",
      "        [[-2.3113, -1.0865, -4.4163, -1.4702, -1.6158, -3.1542, -2.5245]],\n",
      "\n",
      "        [[-2.3513, -1.2391, -6.1401, -1.8622, -1.9065, -1.2204, -4.2757]],\n",
      "\n",
      "        [[-2.0026, -1.0710, -1.8561, -1.8615, -1.8153, -3.3275, -4.4274]]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<LogSoftmaxBackward>)\n",
      "targets_.expand(len(log_probs), -1, -1), tensor([[[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]]], device='cuda:0')\n",
      "log_probs_, tensor([[[-2.9410, -1.0552, -2.9410, -1.4880, -2.9410, -2.8686, -2.9410,\n",
      "          -1.0552]],\n",
      "\n",
      "        [[-3.5908, -1.0294, -3.5908, -1.5933, -3.5908, -2.5154, -3.5908,\n",
      "          -1.0294]],\n",
      "\n",
      "        [[-1.7851, -0.9557, -1.7851, -1.9636, -1.7851, -2.7271, -1.7851,\n",
      "          -0.9557]],\n",
      "\n",
      "        [[-2.3113, -1.0865, -2.3113, -4.4163, -2.3113, -1.4702, -2.3113,\n",
      "          -1.0865]],\n",
      "\n",
      "        [[-2.3513, -1.2391, -2.3513, -6.1401, -2.3513, -1.8622, -2.3513,\n",
      "          -1.2391]],\n",
      "\n",
      "        [[-2.0026, -1.0710, -2.0026, -1.8561, -2.0026, -1.8615, -2.0026,\n",
      "          -1.0710]]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<GatherBackward>)\n",
      "log_probs[0, :, blank], tensor([-2.9410], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "log_probs[0, B, targets_[:, 1]], tensor([-1.0552], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<IndexBackward>)\n",
      "log_alpha.size(), torch.Size([6, 1, 10])\n",
      "log_alpha, tensor([[[-1.7977e+308, -1.7977e+308,  -2.9410e+00,  -1.0552e+00, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<CopySlices>)\n",
      "log_alpha,final: tensor([[[-1.7977e+308, -1.7977e+308,  -2.9410e+00,  -1.0552e+00, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -6.5318e+00,  -1.9433e+00,  -4.6459e+00,\n",
      "           -2.6484e+00, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -8.3169e+00,  -2.8889e+00,  -3.6636e+00,\n",
      "           -3.4616e+00,  -4.4336e+00,  -5.3755e+00, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -1.0628e+01,  -3.9710e+00,  -4.8212e+00,\n",
      "           -6.5997e+00,  -5.4520e+00,  -4.5093e+00,  -7.6868e+00,  -6.4620e+00]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -1.2980e+01,  -5.2088e+00,  -5.9665e+00,\n",
      "           -9.7059e+00,  -7.5277e+00,  -5.9573e+00,  -6.8198e+00,  -5.5798e+00]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -1.4982e+01,  -6.2793e+00,  -6.8270e+00,\n",
      "           -6.6730e+00,  -9.4230e+00,  -7.6105e+00,  -7.6077e+00,  -5.9702e+00]]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<CopySlices>)\n",
      "torch.stack tensor([[[ -1.2980e+01,  -5.2088e+00,  -5.9665e+00,  -9.7059e+00,  -7.5277e+00,\n",
      "           -5.9573e+00,  -6.8198e+00,  -5.5798e+00]],\n",
      "\n",
      "        [[-1.7977e+308,  -1.2980e+01,  -5.2088e+00,  -5.9665e+00,  -9.7059e+00,\n",
      "           -7.5277e+00,  -5.9573e+00,  -6.8198e+00]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308, -1.7977e+308,  -5.2088e+00, -1.7977e+308,\n",
      "           -9.7059e+00, -1.7977e+308,  -5.9573e+00]]], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<StackBackward>)\n",
      "torch.logsumexp(aa, dim = 0) tensor([[-12.9795,  -5.2084,  -4.8244,  -4.8168,  -7.4204,  -5.7490,  -5.6051,\n",
      "          -4.8992]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LogsumexpBackward>)\n",
      "log_alpha, tensor([[[-1.7977e+308, -1.7977e+308,  -2.9410e+00,  -1.0552e+00, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -6.5318e+00,  -1.9433e+00,  -4.6459e+00,\n",
      "           -2.6484e+00, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -8.3169e+00,  -2.8889e+00,  -3.6636e+00,\n",
      "           -3.4616e+00,  -4.4336e+00,  -5.3755e+00, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -1.0628e+01,  -3.9710e+00,  -4.8212e+00,\n",
      "           -6.5997e+00,  -5.4520e+00,  -4.5093e+00,  -7.6868e+00,  -6.4620e+00]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -1.2980e+01,  -5.2088e+00,  -5.9665e+00,\n",
      "           -9.7059e+00,  -7.5277e+00,  -5.9573e+00,  -6.8198e+00,  -5.5798e+00]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -1.4982e+01,  -6.2793e+00,  -6.8270e+00,\n",
      "           -6.6730e+00,  -9.4230e+00,  -7.6105e+00,  -7.6077e+00,  -5.9702e+00]]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<CopySlices>)\n",
      "[zero_padding + target_lengths * 2 - 1, zero_padding + target_lengths * 2], [tensor([7], device='cuda:0'), tensor([8], device='cuda:0')]\n",
      "Custom CTC loss fwd 0.026831626892089844 bwd 0.007383584976196289\n",
      "Custom loss matches: True\n",
      "Grad matches: True\n",
      "CE grad matches: True\n",
      "B, tensor([0], device='cuda:0')\n",
      "targets, tensor([[1, 2, 3]], device='cuda:0')\n",
      "targets_, tensor([[1, 2, 3, 1]], device='cuda:0')\n",
      "targets_, tensor([[0, 1, 0, 2, 0, 3, 0, 1]], device='cuda:0')\n",
      "targets_[:, 2:] != targets_[:, :-2], tensor([[False,  True, False,  True, False,  True]], device='cuda:0')\n",
      "targets_[:, 2:], tensor([[0, 2, 0, 3, 0, 1]], device='cuda:0')\n",
      "targets_[:, :-2], tensor([[0, 1, 0, 2, 0, 3]], device='cuda:0')\n",
      "diff_labels, tensor([[False, False, False,  True, False,  True, False,  True]],\n",
      "       device='cuda:0')\n",
      "zero, -1.7976931348623157e+308\n",
      "log_probs, tensor([[[-2.9410, -1.0552, -1.4880, -2.8686, -3.5114, -1.8952, -1.9928]],\n",
      "\n",
      "        [[-3.5908, -1.0294, -1.5933, -2.5154, -1.3250, -4.8399, -2.8574]],\n",
      "\n",
      "        [[-1.7851, -0.9557, -1.9636, -2.7271, -2.1659, -2.7364, -2.7731]],\n",
      "\n",
      "        [[-2.3113, -1.0865, -4.4163, -1.4702, -1.6158, -3.1542, -2.5245]],\n",
      "\n",
      "        [[-2.3513, -1.2391, -6.1401, -1.8622, -1.9065, -1.2204, -4.2757]],\n",
      "\n",
      "        [[-2.0026, -1.0710, -1.8561, -1.8615, -1.8153, -3.3275, -4.4274]]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<LogSoftmaxBackward>)\n",
      "targets_.expand(len(log_probs), -1, -1), tensor([[[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]]], device='cuda:0')\n",
      "log_probs_, tensor([[[-2.9410, -1.0552, -2.9410, -1.4880, -2.9410, -2.8686, -2.9410,\n",
      "          -1.0552]],\n",
      "\n",
      "        [[-3.5908, -1.0294, -3.5908, -1.5933, -3.5908, -2.5154, -3.5908,\n",
      "          -1.0294]],\n",
      "\n",
      "        [[-1.7851, -0.9557, -1.7851, -1.9636, -1.7851, -2.7271, -1.7851,\n",
      "          -0.9557]],\n",
      "\n",
      "        [[-2.3113, -1.0865, -2.3113, -4.4163, -2.3113, -1.4702, -2.3113,\n",
      "          -1.0865]],\n",
      "\n",
      "        [[-2.3513, -1.2391, -2.3513, -6.1401, -2.3513, -1.8622, -2.3513,\n",
      "          -1.2391]],\n",
      "\n",
      "        [[-2.0026, -1.0710, -2.0026, -1.8561, -2.0026, -1.8615, -2.0026,\n",
      "          -1.0710]]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<GatherBackward>)\n",
      "log_probs[0, :, blank], tensor([-2.9410], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "log_probs[0, B, targets_[:, 1]], tensor([-1.0552], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<IndexBackward>)\n",
      "log_alpha.size(), torch.Size([6, 1, 10])\n",
      "log_alpha, tensor([[[-1.7977e+308, -1.7977e+308,  -2.9410e+00,  -1.0552e+00, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<CopySlices>)\n",
      "log_alpha,final: tensor([[[-1.7977e+308, -1.7977e+308,  -2.9410e+00,  -1.0552e+00, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -6.5318e+00,  -2.0846e+00,  -4.6459e+00,\n",
      "           -2.6484e+00, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -8.3169e+00,  -3.0402e+00,  -3.8697e+00,\n",
      "           -4.0482e+00,  -4.4336e+00,  -5.3755e+00, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -1.0628e+01,  -4.1267e+00,  -5.3515e+00,\n",
      "           -7.4565e+00,  -6.3595e+00,  -5.5184e+00,  -7.6868e+00,  -6.4620e+00]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -1.2980e+01,  -5.3658e+00,  -6.4781e+00,\n",
      "           -1.0267e+01,  -8.7108e+00,  -7.3806e+00,  -7.8697e+00,  -6.7575e+00]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -1.4982e+01,  -6.4368e+00,  -7.3684e+00,\n",
      "           -7.2219e+00,  -1.0713e+01,  -9.2421e+00,  -9.3832e+00,  -7.8285e+00]]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<CopySlices>)\n",
      "torch.stack tensor([[[ -1.2980e+01,  -5.3658e+00,  -6.4781e+00,  -1.0267e+01,  -8.7108e+00,\n",
      "           -7.3806e+00,  -7.8697e+00,  -6.7575e+00]],\n",
      "\n",
      "        [[-1.7977e+308,  -1.2980e+01,  -5.3658e+00,  -6.4781e+00,  -1.0267e+01,\n",
      "           -8.7108e+00,  -7.3806e+00,  -7.8697e+00]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308, -1.7977e+308,  -5.3658e+00, -1.7977e+308,\n",
      "           -1.0267e+01, -1.7977e+308,  -7.3806e+00]]], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<StackBackward>)\n",
      "torch.logsumexp(aa, dim = 0) tensor([[-12.9795,  -5.3653,  -5.0815,  -5.0759,  -8.5194,  -7.1028,  -6.9024,\n",
      "          -6.1342]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LogsumexpBackward>)\n",
      "log_alpha, tensor([[[-1.7977e+308, -1.7977e+308,  -2.9410e+00,  -1.0552e+00, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -6.5318e+00,  -2.0846e+00,  -4.6459e+00,\n",
      "           -2.6484e+00, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -8.3169e+00,  -3.0402e+00,  -3.8697e+00,\n",
      "           -4.0482e+00,  -4.4336e+00,  -5.3755e+00, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -1.0628e+01,  -4.1267e+00,  -5.3515e+00,\n",
      "           -7.4565e+00,  -6.3595e+00,  -5.5184e+00,  -7.6868e+00,  -6.4620e+00]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -1.2980e+01,  -5.3658e+00,  -6.4781e+00,\n",
      "           -1.0267e+01,  -8.7108e+00,  -7.3806e+00,  -7.8697e+00,  -6.7575e+00]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -1.4982e+01,  -6.4368e+00,  -7.3684e+00,\n",
      "           -7.2219e+00,  -1.0713e+01,  -9.2421e+00,  -9.3832e+00,  -7.8285e+00]]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<CopySlices>)\n",
      "[zero_padding + target_lengths * 2 - 1, zero_padding + target_lengths * 2], [tensor([7], device='cuda:0'), tensor([8], device='cuda:0')]\n",
      "path.size, torch.Size([6, 1])\n",
      "path, tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]], device='cuda:0')\n",
      "l1l2.max(dim = -1).indices tensor([0], device='cuda:0')\n",
      "zero_padding + 2 * target_lengths - 1, tensor([7], device='cuda:0')\n",
      "path[input_lengths - 1, B], tensor([7], device='cuda:0')\n",
      "list(enumerate(path))[1:]), [(1, tensor([0], device='cuda:0')), (2, tensor([0], device='cuda:0')), (3, tensor([0], device='cuda:0')), (4, tensor([0], device='cuda:0')), (5, tensor([7], device='cuda:0'))]\n",
      "reversed(list(enumerate(path))[1:]) <list_reverseiterator object at 0x7effec6dd890>\n",
      "(t, indices)= 5 tensor([7], device='cuda:0')\n",
      "diff_labels[B, (indices - zero_padding).clamp(min = 0)], tensor([True], device='cuda:0')\n",
      "(indices - 2) * diff_labels[B, (indices - zero_padding).clamp(min = 0)] tensor([5], device='cuda:0')\n",
      "indices_ tensor([[5, 6, 7]], device='cuda:0')\n",
      "path tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [7],\n",
      "        [7]], device='cuda:0')\n",
      "(t, indices)= 4 tensor([7], device='cuda:0')\n",
      "diff_labels[B, (indices - zero_padding).clamp(min = 0)], tensor([True], device='cuda:0')\n",
      "(indices - 2) * diff_labels[B, (indices - zero_padding).clamp(min = 0)] tensor([5], device='cuda:0')\n",
      "indices_ tensor([[5, 6, 7]], device='cuda:0')\n",
      "path tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [7],\n",
      "        [7],\n",
      "        [7]], device='cuda:0')\n",
      "(t, indices)= 3 tensor([7], device='cuda:0')\n",
      "diff_labels[B, (indices - zero_padding).clamp(min = 0)], tensor([True], device='cuda:0')\n",
      "(indices - 2) * diff_labels[B, (indices - zero_padding).clamp(min = 0)] tensor([5], device='cuda:0')\n",
      "indices_ tensor([[5, 6, 7]], device='cuda:0')\n",
      "path tensor([[0],\n",
      "        [0],\n",
      "        [5],\n",
      "        [7],\n",
      "        [7],\n",
      "        [7]], device='cuda:0')\n",
      "(t, indices)= 2 tensor([5], device='cuda:0')\n",
      "diff_labels[B, (indices - zero_padding).clamp(min = 0)], tensor([True], device='cuda:0')\n",
      "(indices - 2) * diff_labels[B, (indices - zero_padding).clamp(min = 0)] tensor([3], device='cuda:0')\n",
      "indices_ tensor([[3, 4, 5]], device='cuda:0')\n",
      "path tensor([[0],\n",
      "        [3],\n",
      "        [5],\n",
      "        [7],\n",
      "        [7],\n",
      "        [7]], device='cuda:0')\n",
      "(t, indices)= 1 tensor([3], device='cuda:0')\n",
      "diff_labels[B, (indices - zero_padding).clamp(min = 0)], tensor([False], device='cuda:0')\n",
      "(indices - 2) * diff_labels[B, (indices - zero_padding).clamp(min = 0)] tensor([0], device='cuda:0')\n",
      "indices_ tensor([[0, 2, 3]], device='cuda:0')\n",
      "path tensor([[3],\n",
      "        [3],\n",
      "        [5],\n",
      "        [7],\n",
      "        [7],\n",
      "        [7]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test abc-1, new max path\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#@torch.jit.script\n",
    "def ctc_loss(log_probs, targets, input_lengths, target_lengths, blank : int = 0, reduction : str = 'none', alignment : bool = False):\n",
    "\tB = torch.arange(len(targets), device = input_lengths.device)\n",
    "\tprint(\"B,\",B)\n",
    "\tprint(\"targets,\",targets)\n",
    "\ttargets_ = torch.cat([targets, targets[:, :1]], dim = -1)\n",
    "\tprint(\"targets_,\",targets_)\n",
    "\ttargets_ = torch.stack([torch.full_like(targets_, blank), targets_], dim = -1).flatten(start_dim = -2)\n",
    "\tprint(\"targets_,\",targets_)\n",
    "\tprint(\"targets_[:, 2:] != targets_[:, :-2],\", targets_[:, 2:] != targets_[:, :-2])\n",
    "\tprint(\"targets_[:, 2:],\",targets_[:, 2:])\n",
    "\tprint(\"targets_[:, :-2],\",targets_[:, :-2])\n",
    "\tdiff_labels = torch.cat([torch.as_tensor([[False, False]], device = targets.device).expand(len(B), -1), targets_[:, 2:] != targets_[:, :-2]], dim = 1)\n",
    "\tprint(\"diff_labels,\",diff_labels)\n",
    "\t# if the -inf is used as neutral element, custom logsumexp must be used\n",
    "\t#zero = float('-inf')\n",
    "\t# to avoid nan grad in torch.logsumexp\n",
    "\tzero = torch.finfo(log_probs.dtype).min\n",
    "\tprint(\"zero,\",zero)\n",
    "\n",
    "\tzero, zero_padding = torch.tensor(zero, device = log_probs.device, dtype = log_probs.dtype), 2\n",
    "\tprint(\"log_probs,\",log_probs)\n",
    "\tlog_probs_ = log_probs.gather(-1, targets_.expand(len(log_probs), -1, -1))\n",
    "\tprint(\"targets_.expand(len(log_probs), -1, -1),\",targets_.expand(len(log_probs), -1, -1))\n",
    "\tprint(\"log_probs_,\",log_probs_)\n",
    "\tlog_alpha = torch.full((len(log_probs), len(B), zero_padding + targets_.shape[-1]), zero, device = log_probs.device, dtype = log_probs.dtype)\n",
    "\tlog_alpha[0, :, zero_padding + 0] = log_probs[0, :, blank]\n",
    "\tprint(\"log_probs[0, :, blank],\",log_probs[0, :, blank])\n",
    "\tprint(\"log_probs[0, B, targets_[:, 1]],\",log_probs[0, B, targets_[:, 1]])\n",
    "\tlog_alpha[0, :, zero_padding + 1] = log_probs[0, B, targets_[:, 1]]\n",
    "\tprint(\"log_alpha.size(),\",log_alpha.size())\n",
    "\tprint(\"log_alpha,\",log_alpha)\n",
    "\t# log_alpha[1:, :, zero_padding:] = log_probs.gather(-1, targets_.expand(len(log_probs), -1, -1))[1:]\n",
    "\tfor t in range(1, len(log_probs)):\n",
    "\t\tlog_alpha[t, :, 2:] = log_probs_[t] + logadd(log_alpha[t - 1, :, 2:], log_alpha[t - 1, :, 1:-1], torch.where(diff_labels, log_alpha[t - 1, :, :-2], zero))\n",
    "\tprint(\"log_alpha,final:\",log_alpha)\n",
    "\tt=len(log_probs)-1\n",
    "\taa=torch.stack([log_alpha[t - 1, :, 2:], log_alpha[t - 1, :, 1:-1], torch.where(diff_labels, log_alpha[t - 1, :, :-2], zero)])\n",
    "\tprint(\"torch.stack\",aa)\n",
    "\tprint(\"torch.logsumexp(aa, dim = 0)\", torch.logsumexp(aa, dim = 0))\n",
    "\tprint(\"log_alpha,\",log_alpha)\n",
    "\tprint(\"[zero_padding + target_lengths * 2 - 1, zero_padding + target_lengths * 2],\",[zero_padding + target_lengths * 2 - 1, zero_padding + target_lengths * 2])\n",
    "\tl1l2 = log_alpha[input_lengths - 1, B].gather(-1, torch.stack([zero_padding + target_lengths * 2 - 1, zero_padding + target_lengths * 2], dim = -1)) \n",
    "\tloss = -torch.logsumexp(l1l2, dim = -1)\n",
    "\tif not alignment:\n",
    "\t\treturn loss\n",
    "\t\n",
    "\tpath = torch.zeros(len(log_alpha), len(B), device = log_alpha.device, dtype = torch.int64)\n",
    "\tprint(\"path.size,\", path.size())\n",
    "\tprint(\"path,\", path)\n",
    "\tpath[input_lengths - 1, B] = zero_padding + 2 * target_lengths - 1 + l1l2.max(dim = -1).indices\n",
    "\tprint(\"l1l2.max(dim = -1).indices\", l1l2.max(dim = -1).indices)\n",
    "\tprint(\"zero_padding + 2 * target_lengths - 1,\", zero_padding + 2 * target_lengths - 1)\n",
    "\tprint(\"path[input_lengths - 1, B],\",path[input_lengths - 1, B])\n",
    "\tprint(\"list(enumerate(path))[1:]),\",list(enumerate(path))[1:])\n",
    "\tprint(\"reversed(list(enumerate(path))[1:])\",reversed(list(enumerate(path))[1:]))\n",
    "\tfor t, indices in reversed(list(enumerate(path))[1:]):\n",
    "\t\tprint(\"(t, indices)=\",t, indices)\n",
    "\t\tprint(\"diff_labels[B, (indices - zero_padding).clamp(min = 0)],\",diff_labels[B, (indices - zero_padding).clamp(min = 0)])\n",
    "\t\tprint(\"(indices - 2) * diff_labels[B, (indices - zero_padding).clamp(min = 0)]\",(indices - 2) * diff_labels[B, (indices - zero_padding).clamp(min = 0)])\n",
    "\t\tindices_ = torch.stack([(indices - 2) * diff_labels[B, (indices - zero_padding).clamp(min = 0)], (indices - 1).clamp(min = 0), indices], dim = -1)\n",
    "\t\tpath[t - 1] += (indices - 2 + log_alpha[t - 1, B].gather(-1, indices_).max(dim = -1).indices).clamp(min = 0)\n",
    "\t\tprint(\"indices_\",indices_)\n",
    "\t\tprint(\"path\",path)\n",
    "\treturn torch.zeros_like(log_alpha).scatter_(-1, path.unsqueeze(-1), 1.0)[..., (zero_padding + 1)::2]\n",
    "\n",
    "def ctc_max_loss(log_probs, targets, input_lengths, target_lengths, blank : int = 0, reduction : str = 'none', alignment : bool = False):\n",
    "\tB = torch.arange(len(targets), device = input_lengths.device)\n",
    "\tprint(\"B,\",B)\n",
    "\tprint(\"targets,\",targets)\n",
    "\ttargets_ = torch.cat([targets, targets[:, :1]], dim = -1)\n",
    "\tprint(\"targets_,\",targets_)\n",
    "\ttargets_ = torch.stack([torch.full_like(targets_, blank), targets_], dim = -1).flatten(start_dim = -2)\n",
    "\tprint(\"targets_,\",targets_)\n",
    "\tprint(\"targets_[:, 2:] != targets_[:, :-2],\", targets_[:, 2:] != targets_[:, :-2])\n",
    "\tprint(\"targets_[:, 2:],\",targets_[:, 2:])\n",
    "\tprint(\"targets_[:, :-2],\",targets_[:, :-2])\n",
    "\tdiff_labels = torch.cat([torch.as_tensor([[False, False]], device = targets.device).expand(len(B), -1), targets_[:, 2:] != targets_[:, :-2]], dim = 1)\n",
    "\tprint(\"diff_labels,\",diff_labels)\n",
    "\t# if the -inf is used as neutral element, custom logsumexp must be used\n",
    "\t#zero = float('-inf')\n",
    "\t# to avoid nan grad in torch.logsumexp\n",
    "\tzero = torch.finfo(log_probs.dtype).min\n",
    "\tprint(\"zero,\",zero)\n",
    "\n",
    "\tzero, zero_padding = torch.tensor(zero, device = log_probs.device, dtype = log_probs.dtype), 2\n",
    "\tprint(\"log_probs,\",log_probs)\n",
    "\tlog_probs_ = log_probs.gather(-1, targets_.expand(len(log_probs), -1, -1))\n",
    "\tprint(\"targets_.expand(len(log_probs), -1, -1),\",targets_.expand(len(log_probs), -1, -1))\n",
    "\tprint(\"log_probs_,\",log_probs_)\n",
    "\tlog_alpha = torch.full((len(log_probs), len(B), zero_padding + targets_.shape[-1]), zero, device = log_probs.device, dtype = log_probs.dtype)\n",
    "\tlog_alpha[0, :, zero_padding + 0] = log_probs[0, :, blank]\n",
    "\tprint(\"log_probs[0, :, blank],\",log_probs[0, :, blank])\n",
    "\tprint(\"log_probs[0, B, targets_[:, 1]],\",log_probs[0, B, targets_[:, 1]])\n",
    "\tlog_alpha[0, :, zero_padding + 1] = log_probs[0, B, targets_[:, 1]]\n",
    "\tprint(\"log_alpha.size(),\",log_alpha.size())\n",
    "\tprint(\"log_alpha,\",log_alpha)\n",
    "\t# log_alpha[1:, :, zero_padding:] = log_probs.gather(-1, targets_.expand(len(log_probs), -1, -1))[1:]\n",
    "\tfor t in range(1, len(log_probs)):\n",
    "\t\tlog_alpha[t, :, 2:] = log_probs_[t] + logmax(log_alpha[t - 1, :, 2:], log_alpha[t - 1, :, 1:-1], torch.where(diff_labels, log_alpha[t - 1, :, :-2], zero))\n",
    "\tprint(\"log_alpha,final:\",log_alpha)\n",
    "\tt=len(log_probs)-1\n",
    "\taa=torch.stack([log_alpha[t - 1, :, 2:], log_alpha[t - 1, :, 1:-1], torch.where(diff_labels, log_alpha[t - 1, :, :-2], zero)])\n",
    "\tprint(\"torch.stack\",aa)\n",
    "\tprint(\"torch.logsumexp(aa, dim = 0)\", torch.logsumexp(aa, dim = 0))\n",
    "\tprint(\"log_alpha,\",log_alpha)\n",
    "\tprint(\"[zero_padding + target_lengths * 2 - 1, zero_padding + target_lengths * 2],\",[zero_padding + target_lengths * 2 - 1, zero_padding + target_lengths * 2])\n",
    "\tl1l2 = log_alpha[input_lengths - 1, B].gather(-1, torch.stack([zero_padding + target_lengths * 2 - 1, zero_padding + target_lengths * 2], dim = -1)) \n",
    "\tloss = -torch.logsumexp(l1l2, dim = -1)\n",
    "\tif not alignment:\n",
    "\t\treturn loss\n",
    "\t\n",
    "\tpath = torch.zeros(len(log_alpha), len(B), device = log_alpha.device, dtype = torch.int64)\n",
    "\tprint(\"path.size,\", path.size())\n",
    "\tprint(\"path,\", path)\n",
    "\tpath[input_lengths - 1, B] = zero_padding + 2 * target_lengths - 1 + l1l2.max(dim = -1).indices\n",
    "\tprint(\"l1l2.max(dim = -1).indices\", l1l2.max(dim = -1).indices)\n",
    "\tprint(\"zero_padding + 2 * target_lengths - 1,\", zero_padding + 2 * target_lengths - 1)\n",
    "\tprint(\"path[input_lengths - 1, B],\",path[input_lengths - 1, B])\n",
    "\tprint(\"list(enumerate(path))[1:]),\",list(enumerate(path))[1:])\n",
    "\tprint(\"reversed(list(enumerate(path))[1:])\",reversed(list(enumerate(path))[1:]))\n",
    "\tfor t, indices in reversed(list(enumerate(path))[1:]):\n",
    "\t\tprint(\"(t, indices)=\",t, indices)\n",
    "\t\tprint(\"diff_labels[B, (indices - zero_padding).clamp(min = 0)],\",diff_labels[B, (indices - zero_padding).clamp(min = 0)])\n",
    "\t\tprint(\"(indices - 2) * diff_labels[B, (indices - zero_padding).clamp(min = 0)]\",(indices - 2) * diff_labels[B, (indices - zero_padding).clamp(min = 0)])\n",
    "\t\tindices_ = torch.stack([(indices - 2) * diff_labels[B, (indices - zero_padding).clamp(min = 0)], (indices - 1).clamp(min = 0), indices], dim = -1)\n",
    "\t\tpath[t - 1] += (indices - 2 + log_alpha[t - 1, B].gather(-1, indices_).max(dim = -1).indices).clamp(min = 0)\n",
    "\t\tprint(\"indices_\",indices_)\n",
    "\t\tprint(\"path\",path)\n",
    "\treturn torch.zeros_like(log_alpha).scatter_(-1, path.unsqueeze(-1), 1.0)[..., (zero_padding + 1)::2]\n",
    "\n",
    "\n",
    "\n",
    "def ctc_alignment_targets(log_probs, targets, input_lengths, target_lengths, blank = 0, ctc_loss = F.ctc_loss, retain_graph = True):\n",
    "\tloss = ctc_loss(log_probs, targets, input_lengths, target_lengths, blank = blank, reduction = 'sum')\n",
    "\tprobs = log_probs.exp()\n",
    "\t# to simplify API we inline log_softmax gradient, i.e. next two lines are equivalent to: grad_logits, = torch.autograd.grad(loss, logits, retain_graph = True). gradient formula explained at https://stackoverflow.com/questions/35304393/trying-to-understand-code-that-computes-the-gradient-wrt-to-the-input-for-logsof\n",
    "\tgrad_log_probs, = torch.autograd.grad(loss, log_probs, retain_graph = retain_graph)\n",
    "\tgrad_logits = grad_log_probs - probs * grad_log_probs.sum(dim = -1, keepdim = True)\n",
    "\ttemporal_mask = (torch.arange(len(log_probs), device = input_lengths.device, dtype = input_lengths.dtype).unsqueeze(1) < input_lengths.unsqueeze(0)).unsqueeze(-1)\n",
    "\treturn (probs * temporal_mask - grad_logits).detach()\n",
    "\n",
    "def logmax(x0, x1, x2):\n",
    "\t# produces nan gradients in backward if -inf log-space zero element is used https://github.com/pytorch/pytorch/issues/31829\n",
    "# \tprint(\"torch.stack([x0, x1, x2])\",torch.stack([x0, x1, x2]))\n",
    "# \tprint(\"torch.max(torch.stack([x0, x1, x2]), dim = 0\",torch.max(torch.stack([x0, x1, x2]), dim = 0))\n",
    "\treturn torch.max(torch.stack([x0, x1, x2]), dim = 0).values\n",
    "\n",
    "def logadd(x0, x1, x2):\n",
    "\t# produces nan gradients in backward if -inf log-space zero element is used https://github.com/pytorch/pytorch/issues/31829\n",
    "\treturn torch.logsumexp(torch.stack([x0, x1, x2]), dim = 0)\n",
    "\t\n",
    "\t# use if -inf log-space zero element is used\n",
    "\t#return LogsumexpFunction.apply(x0, x1, x2)\n",
    "\t\n",
    "\t# produces inplace modification error https://github.com/pytorch/pytorch/issues/31819\n",
    "\t#m = torch.max(torch.max(x0, x1), x2)\n",
    "\t#m = m.masked_fill(torch.isinf(m), 0)\n",
    "\t#res = (x0 - m).exp() + (x1 - m).exp() + (x2 - m).exp()\n",
    "\t#return res.log().add(m)\n",
    "\n",
    "class LogsumexpFunction(torch.autograd.function.Function):\n",
    "\t@staticmethod\n",
    "\tdef forward(self, x0, x1, x2):\n",
    "\t\tm = torch.max(torch.max(x0, x1), x2)\n",
    "\t\tm = m.masked_fill_(torch.isinf(m), 0)\n",
    "\t\te0 = (x0 - m).exp_()\n",
    "\t\te1 = (x1 - m).exp_()\n",
    "\t\te2 = (x2 - m).exp_()\n",
    "\t\te = (e0 + e1 + e2).clamp_(min = 1e-16)\n",
    "\t\tself.save_for_backward(e0, e1, e2, e)\n",
    "\t\treturn e.log().add_(m)\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef backward(self, grad_output):\n",
    "\t\te0, e1, e2, e = self.saved_tensors\n",
    "\t\tg = grad_output / e\n",
    "\t\treturn g * e0, g * e1, g * e2\n",
    "    \n",
    "probs_seq1 = [[\n",
    "            0.06390443, 0.42124858, 0.27323887, 0.06870235, 0.0361254,\n",
    "            0.18184413, 0.16493624\n",
    "        ], [\n",
    "            0.03309247, 0.42866108, 0.24390638, 0.09699597, 0.31895462,\n",
    "            0.0094893, 0.06890021\n",
    "        ], [\n",
    "            0.218104, 0.49992557, 0.18245131, 0.08503348, 0.14903535,\n",
    "            0.08424043, 0.08120984\n",
    "        ], [\n",
    "            0.12094152, 0.41162472, 0.01473646, 0.28045061, 0.24246305,\n",
    "            0.05206269, 0.09772094\n",
    "        ], [\n",
    "            0.1333387, 0.40550838, 0.00301669, 0.21745861, 0.20803985,\n",
    "            0.41317442, 0.01946335\n",
    "        ], [\n",
    "            0.16468227, 0.4180699, 0.1906545, 0.18963251, 0.19860937,\n",
    "            0.04377724, 0.01457421\n",
    "        ]]\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "torch.set_printoptions(precision=4)\n",
    "\n",
    "\n",
    "# import ctc\n",
    "\n",
    "T, B, C = 6, 1, 7\n",
    "t = 3\n",
    "blank = 0\n",
    "device = 'cuda'\n",
    "seed = 1\n",
    "atol = 1e-3\n",
    "for set_seed in [torch.manual_seed] + ([torch.cuda.manual_seed_all] if device == 'cuda' else []):\n",
    "\tset_seed(seed)\n",
    "tictoc = lambda: (device == 'cuda' and torch.cuda.synchronize()) or time.time()\n",
    "\n",
    "# logits = torch.randn(T, B, C, device = device).requires_grad_()\n",
    "import numpy\n",
    "logits = torch.reshape(torch.as_tensor(numpy.array(numpy.log(probs_seq1)),device = device), (6,1,7)).requires_grad_()\n",
    "\n",
    "# targets = torch.randint(blank + 1, C, (B, t), dtype = torch.long, device = device)\n",
    "targets = torch.reshape(torch.as_tensor(numpy.array([1,2,3]), dtype = torch.long, device = device), (B, t))\n",
    "\n",
    "\n",
    "\n",
    "input_lengths = torch.full((B,), T, dtype = torch.long, device = device)\n",
    "target_lengths = torch.full((B,), t, dtype = torch.long, device = device)\n",
    "log_probs = logits.log_softmax(dim = -1)\n",
    "print(\"log_probs\",log_probs)\n",
    "print('Device:', device)\n",
    "print('Log-probs shape (time X batch X channels):', 'x'.join(map(str, log_probs.shape)))\n",
    "\n",
    "tic = tictoc()\n",
    "builtin_ctc = F.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank = 0, reduction = 'none')\n",
    "toc = tictoc()\n",
    "builtin_ctc_grad, = torch.autograd.grad(builtin_ctc.sum(), logits, retain_graph = True)\n",
    "print('Built-in CTC loss', 'fwd', toc - tic, 'bwd', tictoc() - toc)\n",
    "\n",
    "tic = tictoc()\n",
    "custom_ctc = ctc_loss(log_probs, targets, input_lengths, target_lengths, blank = 0, reduction = 'none')\n",
    "toc = tictoc()\n",
    "custom_ctc_grad, = torch.autograd.grad(custom_ctc.sum(), logits, retain_graph = True)\n",
    "print('Custom CTC loss', 'fwd', toc - tic, 'bwd', tictoc() - toc)\n",
    "\n",
    "ce_alignment_targets = ctc_alignment_targets(log_probs, targets, input_lengths, target_lengths, blank = 0)\n",
    "ce_ctc = -ce_alignment_targets * log_probs\n",
    "ce_ctc_grad, = torch.autograd.grad(ce_ctc.sum(), logits, retain_graph = True)\n",
    "\n",
    "print('Custom loss matches:', torch.allclose(builtin_ctc, custom_ctc, atol = atol))\n",
    "print('Grad matches:', torch.allclose(builtin_ctc_grad, custom_ctc_grad, atol = atol))\n",
    "print('CE grad matches:', torch.allclose(builtin_ctc_grad, ce_ctc_grad, atol = atol))\n",
    "\n",
    "alignment = ctc_max_loss(log_probs, targets, input_lengths, target_lengths, blank = 0, reduction = 'none', alignment = True)\n",
    "a = alignment[:, 0, :target_lengths[0]]\n",
    "# print(a.t().cpu())\n",
    "plt.subplot(211)\n",
    "plt.title('Input-Output Viterbi alignment')\n",
    "plt.imshow(a.t().cpu(), origin = 'lower', aspect = 'auto')\n",
    "plt.xlabel('Input steps')\n",
    "plt.ylabel('Output steps')\n",
    "plt.subplot(212)\n",
    "plt.title('CTC alignment targets')\n",
    "a = ce_alignment_targets[:, 0, :]\n",
    "plt.imshow(a.t().cpu(), origin = 'lower', aspect = 'auto')\n",
    "plt.xlabel('Input steps')\n",
    "plt.ylabel(f'Output symbols, blank {blank}')\n",
    "plt.subplots_adjust(hspace = 0.5)\n",
    "plt.savefig('alignment.png')\n",
    "torch.set_printoptions(precision=2)\n",
    "# print(a.t().cpu())\n",
    "# print(targets[:,0])\n",
    "# print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_probs tensor([[[-2.9410, -1.0552, -1.4880, -2.8686, -3.5114, -1.8952, -1.9928]],\n",
      "\n",
      "        [[-3.5908, -1.0294, -1.5933, -2.5154, -1.3250, -4.8399, -2.8574]],\n",
      "\n",
      "        [[-1.7851, -0.9557, -1.9636, -2.7271, -2.1659, -2.7364, -2.7731]],\n",
      "\n",
      "        [[-2.3113, -1.0865, -4.4163, -1.4702, -1.6158, -3.1542, -2.5245]],\n",
      "\n",
      "        [[-0.8574, -1.6911, -6.5921, -2.3142, -2.3585, -1.6723, -4.7277]],\n",
      "\n",
      "        [[-2.2548, -1.3232, -2.1084, -1.0679, -2.0675, -3.5797, -4.6796]]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<LogSoftmaxBackward>)\n",
      "Device: cuda\n",
      "Log-probs shape (time X batch X channels): 6x1x7\n",
      "Built-in CTC loss fwd 0.0006923675537109375 bwd 0.0007317066192626953\n",
      "B, tensor([0], device='cuda:0')\n",
      "targets, tensor([[1, 2, 3]], device='cuda:0')\n",
      "targets_, tensor([[1, 2, 3, 1]], device='cuda:0')\n",
      "targets_, tensor([[0, 1, 0, 2, 0, 3, 0, 1]], device='cuda:0')\n",
      "targets_[:, 2:] != targets_[:, :-2], tensor([[False,  True, False,  True, False,  True]], device='cuda:0')\n",
      "targets_[:, 2:], tensor([[0, 2, 0, 3, 0, 1]], device='cuda:0')\n",
      "targets_[:, :-2], tensor([[0, 1, 0, 2, 0, 3]], device='cuda:0')\n",
      "diff_labels, tensor([[False, False, False,  True, False,  True, False,  True]],\n",
      "       device='cuda:0')\n",
      "zero, -1.7976931348623157e+308\n",
      "log_probs, tensor([[[-2.9410, -1.0552, -1.4880, -2.8686, -3.5114, -1.8952, -1.9928]],\n",
      "\n",
      "        [[-3.5908, -1.0294, -1.5933, -2.5154, -1.3250, -4.8399, -2.8574]],\n",
      "\n",
      "        [[-1.7851, -0.9557, -1.9636, -2.7271, -2.1659, -2.7364, -2.7731]],\n",
      "\n",
      "        [[-2.3113, -1.0865, -4.4163, -1.4702, -1.6158, -3.1542, -2.5245]],\n",
      "\n",
      "        [[-0.8574, -1.6911, -6.5921, -2.3142, -2.3585, -1.6723, -4.7277]],\n",
      "\n",
      "        [[-2.2548, -1.3232, -2.1084, -1.0679, -2.0675, -3.5797, -4.6796]]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<LogSoftmaxBackward>)\n",
      "targets_.expand(len(log_probs), -1, -1), tensor([[[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]]], device='cuda:0')\n",
      "log_probs_, tensor([[[-2.9410, -1.0552, -2.9410, -1.4880, -2.9410, -2.8686, -2.9410,\n",
      "          -1.0552]],\n",
      "\n",
      "        [[-3.5908, -1.0294, -3.5908, -1.5933, -3.5908, -2.5154, -3.5908,\n",
      "          -1.0294]],\n",
      "\n",
      "        [[-1.7851, -0.9557, -1.7851, -1.9636, -1.7851, -2.7271, -1.7851,\n",
      "          -0.9557]],\n",
      "\n",
      "        [[-2.3113, -1.0865, -2.3113, -4.4163, -2.3113, -1.4702, -2.3113,\n",
      "          -1.0865]],\n",
      "\n",
      "        [[-0.8574, -1.6911, -0.8574, -6.5921, -0.8574, -2.3142, -0.8574,\n",
      "          -1.6911]],\n",
      "\n",
      "        [[-2.2548, -1.3232, -2.2548, -2.1084, -2.2548, -1.0679, -2.2548,\n",
      "          -1.3232]]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<GatherBackward>)\n",
      "log_probs[0, :, blank], tensor([-2.9410], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "log_probs[0, B, targets_[:, 1]], tensor([-1.0552], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<IndexBackward>)\n",
      "log_alpha.size(), torch.Size([6, 1, 10])\n",
      "log_alpha, tensor([[[-1.7977e+308, -1.7977e+308,  -2.9410e+00,  -1.0552e+00, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<CopySlices>)\n",
      "log_alpha,final: tensor([[[-1.7977e+308, -1.7977e+308,  -2.9410e+00,  -1.0552e+00, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -6.5318e+00,  -1.9433e+00,  -4.6459e+00,\n",
      "           -2.6484e+00, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -8.3169e+00,  -2.8889e+00,  -3.6636e+00,\n",
      "           -3.4616e+00,  -4.4336e+00,  -5.3755e+00, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -1.0628e+01,  -3.9710e+00,  -4.8212e+00,\n",
      "           -6.5997e+00,  -5.4520e+00,  -4.5093e+00,  -7.6868e+00,  -6.4620e+00]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -1.1486e+01,  -5.6608e+00,  -4.4726e+00,\n",
      "           -1.0158e+01,  -6.0338e+00,  -6.4092e+00,  -5.3259e+00,  -6.0318e+00]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -1.3740e+01,  -6.9810e+00,  -6.4614e+00,\n",
      "           -6.3123e+00,  -8.2726e+00,  -6.5693e+00,  -7.2892e+00,  -6.0436e+00]]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<CopySlices>)\n",
      "torch.stack tensor([[[ -1.1486e+01,  -5.6608e+00,  -4.4726e+00,  -1.0158e+01,  -6.0338e+00,\n",
      "           -6.4092e+00,  -5.3259e+00,  -6.0318e+00]],\n",
      "\n",
      "        [[-1.7977e+308,  -1.1486e+01,  -5.6608e+00,  -4.4726e+00,  -1.0158e+01,\n",
      "           -6.0338e+00,  -6.4092e+00,  -5.3259e+00]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308, -1.7977e+308,  -5.6608e+00, -1.7977e+308,\n",
      "           -1.0158e+01, -1.7977e+308,  -6.4092e+00]]], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<StackBackward>)\n",
      "torch.logsumexp(aa, dim = 0) tensor([[-11.4856,  -5.6578,  -4.2066,  -4.2040,  -6.0178,  -5.5013,  -5.0343,\n",
      "          -4.7204]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LogsumexpBackward>)\n",
      "log_alpha, tensor([[[-1.7977e+308, -1.7977e+308,  -2.9410e+00,  -1.0552e+00, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -6.5318e+00,  -1.9433e+00,  -4.6459e+00,\n",
      "           -2.6484e+00, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -8.3169e+00,  -2.8889e+00,  -3.6636e+00,\n",
      "           -3.4616e+00,  -4.4336e+00,  -5.3755e+00, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -1.0628e+01,  -3.9710e+00,  -4.8212e+00,\n",
      "           -6.5997e+00,  -5.4520e+00,  -4.5093e+00,  -7.6868e+00,  -6.4620e+00]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -1.1486e+01,  -5.6608e+00,  -4.4726e+00,\n",
      "           -1.0158e+01,  -6.0338e+00,  -6.4092e+00,  -5.3259e+00,  -6.0318e+00]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -1.3740e+01,  -6.9810e+00,  -6.4614e+00,\n",
      "           -6.3123e+00,  -8.2726e+00,  -6.5693e+00,  -7.2892e+00,  -6.0436e+00]]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<CopySlices>)\n",
      "[zero_padding + target_lengths * 2 - 1, zero_padding + target_lengths * 2], [tensor([7], device='cuda:0'), tensor([8], device='cuda:0')]\n",
      "Custom CTC loss fwd 0.025357961654663086 bwd 0.007323265075683594\n",
      "Custom loss matches: True\n",
      "Grad matches: True\n",
      "CE grad matches: True\n",
      "B, tensor([0], device='cuda:0')\n",
      "targets, tensor([[1, 2, 3]], device='cuda:0')\n",
      "targets_, tensor([[1, 2, 3, 1]], device='cuda:0')\n",
      "targets_, tensor([[0, 1, 0, 2, 0, 3, 0, 1]], device='cuda:0')\n",
      "targets_[:, 2:] != targets_[:, :-2], tensor([[False,  True, False,  True, False,  True]], device='cuda:0')\n",
      "targets_[:, 2:], tensor([[0, 2, 0, 3, 0, 1]], device='cuda:0')\n",
      "targets_[:, :-2], tensor([[0, 1, 0, 2, 0, 3]], device='cuda:0')\n",
      "diff_labels, tensor([[False, False, False,  True, False,  True, False,  True]],\n",
      "       device='cuda:0')\n",
      "zero, -1.7976931348623157e+308\n",
      "log_probs, tensor([[[-2.9410, -1.0552, -1.4880, -2.8686, -3.5114, -1.8952, -1.9928]],\n",
      "\n",
      "        [[-3.5908, -1.0294, -1.5933, -2.5154, -1.3250, -4.8399, -2.8574]],\n",
      "\n",
      "        [[-1.7851, -0.9557, -1.9636, -2.7271, -2.1659, -2.7364, -2.7731]],\n",
      "\n",
      "        [[-2.3113, -1.0865, -4.4163, -1.4702, -1.6158, -3.1542, -2.5245]],\n",
      "\n",
      "        [[-0.8574, -1.6911, -6.5921, -2.3142, -2.3585, -1.6723, -4.7277]],\n",
      "\n",
      "        [[-2.2548, -1.3232, -2.1084, -1.0679, -2.0675, -3.5797, -4.6796]]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<LogSoftmaxBackward>)\n",
      "targets_.expand(len(log_probs), -1, -1), tensor([[[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]]], device='cuda:0')\n",
      "log_probs_, tensor([[[-2.9410, -1.0552, -2.9410, -1.4880, -2.9410, -2.8686, -2.9410,\n",
      "          -1.0552]],\n",
      "\n",
      "        [[-3.5908, -1.0294, -3.5908, -1.5933, -3.5908, -2.5154, -3.5908,\n",
      "          -1.0294]],\n",
      "\n",
      "        [[-1.7851, -0.9557, -1.7851, -1.9636, -1.7851, -2.7271, -1.7851,\n",
      "          -0.9557]],\n",
      "\n",
      "        [[-2.3113, -1.0865, -2.3113, -4.4163, -2.3113, -1.4702, -2.3113,\n",
      "          -1.0865]],\n",
      "\n",
      "        [[-0.8574, -1.6911, -0.8574, -6.5921, -0.8574, -2.3142, -0.8574,\n",
      "          -1.6911]],\n",
      "\n",
      "        [[-2.2548, -1.3232, -2.2548, -2.1084, -2.2548, -1.0679, -2.2548,\n",
      "          -1.3232]]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<GatherBackward>)\n",
      "log_probs[0, :, blank], tensor([-2.9410], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "log_probs[0, B, targets_[:, 1]], tensor([-1.0552], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<IndexBackward>)\n",
      "log_alpha.size(), torch.Size([6, 1, 10])\n",
      "log_alpha, tensor([[[-1.7977e+308, -1.7977e+308,  -2.9410e+00,  -1.0552e+00, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<CopySlices>)\n",
      "log_alpha,final: tensor([[[-1.7977e+308, -1.7977e+308,  -2.9410e+00,  -1.0552e+00, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -6.5318e+00,  -2.0846e+00,  -4.6459e+00,\n",
      "           -2.6484e+00, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -8.3169e+00,  -3.0402e+00,  -3.8697e+00,\n",
      "           -4.0482e+00,  -4.4336e+00,  -5.3755e+00, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -1.0628e+01,  -4.1267e+00,  -5.3515e+00,\n",
      "           -7.4565e+00,  -6.3595e+00,  -5.5184e+00,  -7.6868e+00,  -6.4620e+00]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -1.1486e+01,  -5.8178e+00,  -4.9842e+00,\n",
      "           -1.0719e+01,  -7.2169e+00,  -7.8326e+00,  -6.3759e+00,  -7.2095e+00]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -1.3740e+01,  -7.1410e+00,  -7.2390e+00,\n",
      "           -7.0925e+00,  -9.4718e+00,  -8.2849e+00,  -8.6307e+00,  -7.6990e+00]]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<CopySlices>)\n",
      "torch.stack tensor([[[ -1.1486e+01,  -5.8178e+00,  -4.9842e+00,  -1.0719e+01,  -7.2169e+00,\n",
      "           -7.8326e+00,  -6.3759e+00,  -7.2095e+00]],\n",
      "\n",
      "        [[-1.7977e+308,  -1.1486e+01,  -5.8178e+00,  -4.9842e+00,  -1.0719e+01,\n",
      "           -7.2169e+00,  -7.8326e+00,  -6.3759e+00]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308, -1.7977e+308,  -5.8178e+00, -1.7977e+308,\n",
      "           -1.0719e+01, -1.7977e+308,  -7.8326e+00]]], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<StackBackward>)\n",
      "torch.logsumexp(aa, dim = 0) tensor([[-11.4856,  -5.8143,  -4.6234,  -4.6211,  -7.1872,  -6.7656,  -6.1664,\n",
      "          -5.8645]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LogsumexpBackward>)\n",
      "log_alpha, tensor([[[-1.7977e+308, -1.7977e+308,  -2.9410e+00,  -1.0552e+00, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -6.5318e+00,  -2.0846e+00,  -4.6459e+00,\n",
      "           -2.6484e+00, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -8.3169e+00,  -3.0402e+00,  -3.8697e+00,\n",
      "           -4.0482e+00,  -4.4336e+00,  -5.3755e+00, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -1.0628e+01,  -4.1267e+00,  -5.3515e+00,\n",
      "           -7.4565e+00,  -6.3595e+00,  -5.5184e+00,  -7.6868e+00,  -6.4620e+00]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -1.1486e+01,  -5.8178e+00,  -4.9842e+00,\n",
      "           -1.0719e+01,  -7.2169e+00,  -7.8326e+00,  -6.3759e+00,  -7.2095e+00]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -1.3740e+01,  -7.1410e+00,  -7.2390e+00,\n",
      "           -7.0925e+00,  -9.4718e+00,  -8.2849e+00,  -8.6307e+00,  -7.6990e+00]]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<CopySlices>)\n",
      "[zero_padding + target_lengths * 2 - 1, zero_padding + target_lengths * 2], [tensor([7], device='cuda:0'), tensor([8], device='cuda:0')]\n",
      "path.size, torch.Size([6, 1])\n",
      "path, tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]], device='cuda:0')\n",
      "l1l2.max(dim = -1).indices tensor([0], device='cuda:0')\n",
      "zero_padding + 2 * target_lengths - 1, tensor([7], device='cuda:0')\n",
      "path[input_lengths - 1, B], tensor([7], device='cuda:0')\n",
      "list(enumerate(path))[1:]), [(1, tensor([0], device='cuda:0')), (2, tensor([0], device='cuda:0')), (3, tensor([0], device='cuda:0')), (4, tensor([0], device='cuda:0')), (5, tensor([7], device='cuda:0'))]\n",
      "reversed(list(enumerate(path))[1:]) <list_reverseiterator object at 0x7eff74191290>\n",
      "(t, indices)= 5 tensor([7], device='cuda:0')\n",
      "diff_labels[B, (indices - zero_padding).clamp(min = 0)], tensor([True], device='cuda:0')\n",
      "(indices - 2) * diff_labels[B, (indices - zero_padding).clamp(min = 0)] tensor([5], device='cuda:0')\n",
      "indices_ tensor([[5, 6, 7]], device='cuda:0')\n",
      "path tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [6],\n",
      "        [7]], device='cuda:0')\n",
      "(t, indices)= 4 tensor([6], device='cuda:0')\n",
      "diff_labels[B, (indices - zero_padding).clamp(min = 0)], tensor([False], device='cuda:0')\n",
      "(indices - 2) * diff_labels[B, (indices - zero_padding).clamp(min = 0)] tensor([0], device='cuda:0')\n",
      "indices_ tensor([[0, 5, 6]], device='cuda:0')\n",
      "path tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [6],\n",
      "        [6],\n",
      "        [7]], device='cuda:0')\n",
      "(t, indices)= 3 tensor([6], device='cuda:0')\n",
      "diff_labels[B, (indices - zero_padding).clamp(min = 0)], tensor([False], device='cuda:0')\n",
      "(indices - 2) * diff_labels[B, (indices - zero_padding).clamp(min = 0)] tensor([0], device='cuda:0')\n",
      "indices_ tensor([[0, 5, 6]], device='cuda:0')\n",
      "path tensor([[0],\n",
      "        [0],\n",
      "        [5],\n",
      "        [6],\n",
      "        [6],\n",
      "        [7]], device='cuda:0')\n",
      "(t, indices)= 2 tensor([5], device='cuda:0')\n",
      "diff_labels[B, (indices - zero_padding).clamp(min = 0)], tensor([True], device='cuda:0')\n",
      "(indices - 2) * diff_labels[B, (indices - zero_padding).clamp(min = 0)] tensor([3], device='cuda:0')\n",
      "indices_ tensor([[3, 4, 5]], device='cuda:0')\n",
      "path tensor([[0],\n",
      "        [3],\n",
      "        [5],\n",
      "        [6],\n",
      "        [6],\n",
      "        [7]], device='cuda:0')\n",
      "(t, indices)= 1 tensor([3], device='cuda:0')\n",
      "diff_labels[B, (indices - zero_padding).clamp(min = 0)], tensor([False], device='cuda:0')\n",
      "(indices - 2) * diff_labels[B, (indices - zero_padding).clamp(min = 0)] tensor([0], device='cuda:0')\n",
      "indices_ tensor([[0, 2, 3]], device='cuda:0')\n",
      "path tensor([[3],\n",
      "        [3],\n",
      "        [5],\n",
      "        [6],\n",
      "        [6],\n",
      "        [7]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test abc-3, max path\n",
    "\n",
    "probs_seq1 = [[\n",
    "            0.06390443, 0.42124858, 0.27323887, 0.06870235, 0.0361254,\n",
    "            0.18184413, 0.16493624\n",
    "        ], [\n",
    "            0.03309247, 0.42866108, 0.24390638, 0.09699597, 0.31895462,\n",
    "            0.0094893, 0.06890021\n",
    "        ], [\n",
    "            0.218104, 0.49992557, 0.18245131, 0.08503348, 0.14903535,\n",
    "            0.08424043, 0.08120984\n",
    "        ], [\n",
    "            0.12094152, 0.41162472, 0.01473646, 0.28045061, 0.24246305,\n",
    "            0.05206269, 0.09772094\n",
    "        ], [\n",
    "            0.9333387, 0.40550838, 0.00301669, 0.21745861, 0.20803985,\n",
    "            0.41317442, 0.01946335\n",
    "        ], [\n",
    "            0.16468227, 0.4180699, 0.1906545, 0.53963251, 0.19860937,\n",
    "            0.04377724, 0.01457421\n",
    "        ]]\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "torch.set_printoptions(precision=4)\n",
    "\n",
    "\n",
    "# import ctc\n",
    "\n",
    "T, B, C = 6, 1, 7\n",
    "t = 3\n",
    "blank = 0\n",
    "device = 'cuda'\n",
    "seed = 1\n",
    "atol = 1e-3\n",
    "for set_seed in [torch.manual_seed] + ([torch.cuda.manual_seed_all] if device == 'cuda' else []):\n",
    "\tset_seed(seed)\n",
    "tictoc = lambda: (device == 'cuda' and torch.cuda.synchronize()) or time.time()\n",
    "\n",
    "# logits = torch.randn(T, B, C, device = device).requires_grad_()\n",
    "import numpy\n",
    "logits = torch.reshape(torch.as_tensor(numpy.array(numpy.log(probs_seq1)),device = device), (6,1,7)).requires_grad_()\n",
    "\n",
    "# targets = torch.randint(blank + 1, C, (B, t), dtype = torch.long, device = device)\n",
    "targets = torch.reshape(torch.as_tensor(numpy.array([1,2,3]), dtype = torch.long, device = device), (B, t))\n",
    "\n",
    "\n",
    "\n",
    "input_lengths = torch.full((B,), T, dtype = torch.long, device = device)\n",
    "target_lengths = torch.full((B,), t, dtype = torch.long, device = device)\n",
    "log_probs = logits.log_softmax(dim = -1)\n",
    "print(\"log_probs\",log_probs)\n",
    "print('Device:', device)\n",
    "print('Log-probs shape (time X batch X channels):', 'x'.join(map(str, log_probs.shape)))\n",
    "\n",
    "tic = tictoc()\n",
    "builtin_ctc = F.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank = 0, reduction = 'none')\n",
    "toc = tictoc()\n",
    "builtin_ctc_grad, = torch.autograd.grad(builtin_ctc.sum(), logits, retain_graph = True)\n",
    "print('Built-in CTC loss', 'fwd', toc - tic, 'bwd', tictoc() - toc)\n",
    "\n",
    "tic = tictoc()\n",
    "custom_ctc = ctc_loss(log_probs, targets, input_lengths, target_lengths, blank = 0, reduction = 'none')\n",
    "toc = tictoc()\n",
    "custom_ctc_grad, = torch.autograd.grad(custom_ctc.sum(), logits, retain_graph = True)\n",
    "print('Custom CTC loss', 'fwd', toc - tic, 'bwd', tictoc() - toc)\n",
    "\n",
    "ce_alignment_targets = ctc_alignment_targets(log_probs, targets, input_lengths, target_lengths, blank = 0)\n",
    "ce_ctc = -ce_alignment_targets * log_probs\n",
    "ce_ctc_grad, = torch.autograd.grad(ce_ctc.sum(), logits, retain_graph = True)\n",
    "\n",
    "print('Custom loss matches:', torch.allclose(builtin_ctc, custom_ctc, atol = atol))\n",
    "print('Grad matches:', torch.allclose(builtin_ctc_grad, custom_ctc_grad, atol = atol))\n",
    "print('CE grad matches:', torch.allclose(builtin_ctc_grad, ce_ctc_grad, atol = atol))\n",
    "\n",
    "alignment = ctc_max_loss(log_probs, targets, input_lengths, target_lengths, blank = 0, reduction = 'none', alignment = True)\n",
    "a = alignment[:, 0, :target_lengths[0]]\n",
    "# print(a.t().cpu())\n",
    "plt.subplot(211)\n",
    "plt.title('Input-Output Viterbi alignment')\n",
    "plt.imshow(a.t().cpu(), origin = 'lower', aspect = 'auto')\n",
    "plt.xlabel('Input steps')\n",
    "plt.ylabel('Output steps')\n",
    "plt.subplot(212)\n",
    "plt.title('CTC alignment targets')\n",
    "a = ce_alignment_targets[:, 0, :]\n",
    "plt.imshow(a.t().cpu(), origin = 'lower', aspect = 'auto')\n",
    "plt.xlabel('Input steps')\n",
    "plt.ylabel(f'Output symbols, blank {blank}')\n",
    "plt.subplots_adjust(hspace = 0.5)\n",
    "plt.savefig('alignment.png')\n",
    "torch.set_printoptions(precision=2)\n",
    "# print(a.t().cpu())\n",
    "# print(targets[:,0])\n",
    "# print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_probs tensor([[[-2.9410, -1.0552, -1.4880, -2.8686, -3.5114, -1.8952, -1.9928]],\n",
      "\n",
      "        [[-3.5908, -1.0294, -1.5933, -2.5154, -1.3250, -4.8399, -2.8574]],\n",
      "\n",
      "        [[-1.7851, -0.9557, -1.9636, -2.7271, -2.1659, -2.7364, -2.7731]],\n",
      "\n",
      "        [[-2.3113, -1.0865, -4.4163, -1.4702, -1.6158, -3.1542, -2.5245]],\n",
      "\n",
      "        [[-1.2164, -1.4904, -6.3914, -2.1135, -2.1578, -1.4717, -4.5270]],\n",
      "\n",
      "        [[-2.2548, -1.3232, -2.1084, -1.0679, -2.0675, -3.5797, -4.6796]]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<LogSoftmaxBackward>)\n",
      "Device: cuda\n",
      "Log-probs shape (time X batch X channels): 6x1x7\n",
      "Built-in CTC loss fwd 0.00042128562927246094 bwd 0.0007760524749755859\n",
      "B, tensor([0], device='cuda:0')\n",
      "targets, tensor([[1, 2, 3]], device='cuda:0')\n",
      "targets_, tensor([[1, 2, 3, 1]], device='cuda:0')\n",
      "targets_, tensor([[0, 1, 0, 2, 0, 3, 0, 1]], device='cuda:0')\n",
      "targets_[:, 2:] != targets_[:, :-2], tensor([[False,  True, False,  True, False,  True]], device='cuda:0')\n",
      "targets_[:, 2:], tensor([[0, 2, 0, 3, 0, 1]], device='cuda:0')\n",
      "targets_[:, :-2], tensor([[0, 1, 0, 2, 0, 3]], device='cuda:0')\n",
      "diff_labels, tensor([[False, False, False,  True, False,  True, False,  True]],\n",
      "       device='cuda:0')\n",
      "zero, -1.7976931348623157e+308\n",
      "log_probs, tensor([[[-2.9410, -1.0552, -1.4880, -2.8686, -3.5114, -1.8952, -1.9928]],\n",
      "\n",
      "        [[-3.5908, -1.0294, -1.5933, -2.5154, -1.3250, -4.8399, -2.8574]],\n",
      "\n",
      "        [[-1.7851, -0.9557, -1.9636, -2.7271, -2.1659, -2.7364, -2.7731]],\n",
      "\n",
      "        [[-2.3113, -1.0865, -4.4163, -1.4702, -1.6158, -3.1542, -2.5245]],\n",
      "\n",
      "        [[-1.2164, -1.4904, -6.3914, -2.1135, -2.1578, -1.4717, -4.5270]],\n",
      "\n",
      "        [[-2.2548, -1.3232, -2.1084, -1.0679, -2.0675, -3.5797, -4.6796]]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<LogSoftmaxBackward>)\n",
      "targets_.expand(len(log_probs), -1, -1), tensor([[[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]]], device='cuda:0')\n",
      "log_probs_, tensor([[[-2.9410, -1.0552, -2.9410, -1.4880, -2.9410, -2.8686, -2.9410,\n",
      "          -1.0552]],\n",
      "\n",
      "        [[-3.5908, -1.0294, -3.5908, -1.5933, -3.5908, -2.5154, -3.5908,\n",
      "          -1.0294]],\n",
      "\n",
      "        [[-1.7851, -0.9557, -1.7851, -1.9636, -1.7851, -2.7271, -1.7851,\n",
      "          -0.9557]],\n",
      "\n",
      "        [[-2.3113, -1.0865, -2.3113, -4.4163, -2.3113, -1.4702, -2.3113,\n",
      "          -1.0865]],\n",
      "\n",
      "        [[-1.2164, -1.4904, -1.2164, -6.3914, -1.2164, -2.1135, -1.2164,\n",
      "          -1.4904]],\n",
      "\n",
      "        [[-2.2548, -1.3232, -2.2548, -2.1084, -2.2548, -1.0679, -2.2548,\n",
      "          -1.3232]]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<GatherBackward>)\n",
      "log_probs[0, :, blank], tensor([-2.9410], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "log_probs[0, B, targets_[:, 1]], tensor([-1.0552], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<IndexBackward>)\n",
      "log_alpha.size(), torch.Size([6, 1, 10])\n",
      "log_alpha, tensor([[[-1.7977e+308, -1.7977e+308,  -2.9410e+00,  -1.0552e+00, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<CopySlices>)\n",
      "log_alpha,final: tensor([[[-1.7977e+308, -1.7977e+308,  -2.9410e+00,  -1.0552e+00, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -6.5318e+00,  -1.9433e+00,  -4.6459e+00,\n",
      "           -2.6484e+00, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -8.3169e+00,  -2.8889e+00,  -3.6636e+00,\n",
      "           -3.4616e+00,  -4.4336e+00,  -5.3755e+00, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -1.0628e+01,  -3.9710e+00,  -4.8212e+00,\n",
      "           -6.5997e+00,  -5.4520e+00,  -4.5093e+00,  -7.6868e+00,  -6.4620e+00]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -1.1845e+01,  -5.4601e+00,  -4.8316e+00,\n",
      "           -9.9572e+00,  -6.3928e+00,  -6.2086e+00,  -5.6848e+00,  -5.8311e+00]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -1.4099e+01,  -6.7816e+00,  -6.6589e+00,\n",
      "           -6.5086e+00,  -8.6196e+00,  -6.6584e+00,  -7.4744e+00,  -6.1094e+00]]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<CopySlices>)\n",
      "torch.stack tensor([[[ -1.1845e+01,  -5.4601e+00,  -4.8316e+00,  -9.9572e+00,  -6.3928e+00,\n",
      "           -6.2086e+00,  -5.6848e+00,  -5.8311e+00]],\n",
      "\n",
      "        [[-1.7977e+308,  -1.1845e+01,  -5.4601e+00,  -4.8316e+00,  -9.9572e+00,\n",
      "           -6.3928e+00,  -6.2086e+00,  -5.6848e+00]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308, -1.7977e+308,  -5.4601e+00, -1.7977e+308,\n",
      "           -9.9572e+00, -1.7977e+308,  -6.2086e+00]]], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<StackBackward>)\n",
      "torch.logsumexp(aa, dim = 0) tensor([[-11.8446,  -5.4584,  -4.4041,  -4.4002,  -6.3648,  -5.5905,  -5.2196,\n",
      "          -4.7862]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LogsumexpBackward>)\n",
      "log_alpha, tensor([[[-1.7977e+308, -1.7977e+308,  -2.9410e+00,  -1.0552e+00, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -6.5318e+00,  -1.9433e+00,  -4.6459e+00,\n",
      "           -2.6484e+00, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -8.3169e+00,  -2.8889e+00,  -3.6636e+00,\n",
      "           -3.4616e+00,  -4.4336e+00,  -5.3755e+00, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -1.0628e+01,  -3.9710e+00,  -4.8212e+00,\n",
      "           -6.5997e+00,  -5.4520e+00,  -4.5093e+00,  -7.6868e+00,  -6.4620e+00]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -1.1845e+01,  -5.4601e+00,  -4.8316e+00,\n",
      "           -9.9572e+00,  -6.3928e+00,  -6.2086e+00,  -5.6848e+00,  -5.8311e+00]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -1.4099e+01,  -6.7816e+00,  -6.6589e+00,\n",
      "           -6.5086e+00,  -8.6196e+00,  -6.6584e+00,  -7.4744e+00,  -6.1094e+00]]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<CopySlices>)\n",
      "[zero_padding + target_lengths * 2 - 1, zero_padding + target_lengths * 2], [tensor([7], device='cuda:0'), tensor([8], device='cuda:0')]\n",
      "Custom CTC loss fwd 0.03530097007751465 bwd 0.007495403289794922\n",
      "Custom loss matches: True\n",
      "Grad matches: True\n",
      "CE grad matches: True\n",
      "B, tensor([0], device='cuda:0')\n",
      "targets, tensor([[1, 2, 3]], device='cuda:0')\n",
      "targets_, tensor([[1, 2, 3, 1]], device='cuda:0')\n",
      "targets_, tensor([[0, 1, 0, 2, 0, 3, 0, 1]], device='cuda:0')\n",
      "targets_[:, 2:] != targets_[:, :-2], tensor([[False,  True, False,  True, False,  True]], device='cuda:0')\n",
      "targets_[:, 2:], tensor([[0, 2, 0, 3, 0, 1]], device='cuda:0')\n",
      "targets_[:, :-2], tensor([[0, 1, 0, 2, 0, 3]], device='cuda:0')\n",
      "diff_labels, tensor([[False, False, False,  True, False,  True, False,  True]],\n",
      "       device='cuda:0')\n",
      "zero, -1.7976931348623157e+308\n",
      "log_probs, tensor([[[-2.9410, -1.0552, -1.4880, -2.8686, -3.5114, -1.8952, -1.9928]],\n",
      "\n",
      "        [[-3.5908, -1.0294, -1.5933, -2.5154, -1.3250, -4.8399, -2.8574]],\n",
      "\n",
      "        [[-1.7851, -0.9557, -1.9636, -2.7271, -2.1659, -2.7364, -2.7731]],\n",
      "\n",
      "        [[-2.3113, -1.0865, -4.4163, -1.4702, -1.6158, -3.1542, -2.5245]],\n",
      "\n",
      "        [[-1.2164, -1.4904, -6.3914, -2.1135, -2.1578, -1.4717, -4.5270]],\n",
      "\n",
      "        [[-2.2548, -1.3232, -2.1084, -1.0679, -2.0675, -3.5797, -4.6796]]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<LogSoftmaxBackward>)\n",
      "targets_.expand(len(log_probs), -1, -1), tensor([[[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]]], device='cuda:0')\n",
      "log_probs_, tensor([[[-2.9410, -1.0552, -2.9410, -1.4880, -2.9410, -2.8686, -2.9410,\n",
      "          -1.0552]],\n",
      "\n",
      "        [[-3.5908, -1.0294, -3.5908, -1.5933, -3.5908, -2.5154, -3.5908,\n",
      "          -1.0294]],\n",
      "\n",
      "        [[-1.7851, -0.9557, -1.7851, -1.9636, -1.7851, -2.7271, -1.7851,\n",
      "          -0.9557]],\n",
      "\n",
      "        [[-2.3113, -1.0865, -2.3113, -4.4163, -2.3113, -1.4702, -2.3113,\n",
      "          -1.0865]],\n",
      "\n",
      "        [[-1.2164, -1.4904, -1.2164, -6.3914, -1.2164, -2.1135, -1.2164,\n",
      "          -1.4904]],\n",
      "\n",
      "        [[-2.2548, -1.3232, -2.2548, -2.1084, -2.2548, -1.0679, -2.2548,\n",
      "          -1.3232]]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<GatherBackward>)\n",
      "log_probs[0, :, blank], tensor([-2.9410], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "log_probs[0, B, targets_[:, 1]], tensor([-1.0552], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<IndexBackward>)\n",
      "log_alpha.size(), torch.Size([6, 1, 10])\n",
      "log_alpha, tensor([[[-1.7977e+308, -1.7977e+308,  -2.9410e+00,  -1.0552e+00, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<CopySlices>)\n",
      "log_alpha,final: tensor([[[-1.7977e+308, -1.7977e+308,  -2.9410e+00,  -1.0552e+00, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -6.5318e+00,  -2.0846e+00,  -4.6459e+00,\n",
      "           -2.6484e+00, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -8.3169e+00,  -3.0402e+00,  -3.8697e+00,\n",
      "           -4.0482e+00,  -4.4336e+00,  -5.3755e+00, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -1.0628e+01,  -4.1267e+00,  -5.3515e+00,\n",
      "           -7.4565e+00,  -6.3595e+00,  -5.5184e+00,  -7.6868e+00,  -6.4620e+00]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -1.1845e+01,  -5.6171e+00,  -5.3431e+00,\n",
      "           -1.0518e+01,  -7.5759e+00,  -7.6319e+00,  -6.7348e+00,  -7.0088e+00]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -1.4099e+01,  -6.9403e+00,  -7.5979e+00,\n",
      "           -7.4515e+00,  -9.8307e+00,  -8.6438e+00,  -8.9896e+00,  -8.0580e+00]]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<CopySlices>)\n",
      "torch.stack tensor([[[ -1.1845e+01,  -5.6171e+00,  -5.3431e+00,  -1.0518e+01,  -7.5759e+00,\n",
      "           -7.6319e+00,  -6.7348e+00,  -7.0088e+00]],\n",
      "\n",
      "        [[-1.7977e+308,  -1.1845e+01,  -5.6171e+00,  -5.3431e+00,  -1.0518e+01,\n",
      "           -7.5759e+00,  -7.6319e+00,  -6.7348e+00]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308, -1.7977e+308,  -5.6171e+00, -1.7977e+308,\n",
      "           -1.0518e+01, -1.7977e+308,  -7.6319e+00]]], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<StackBackward>)\n",
      "torch.logsumexp(aa, dim = 0) tensor([[-11.8446,  -5.6151,  -4.7776,  -4.7744,  -7.5245,  -6.8836,  -6.3928,\n",
      "          -5.9610]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LogsumexpBackward>)\n",
      "log_alpha, tensor([[[-1.7977e+308, -1.7977e+308,  -2.9410e+00,  -1.0552e+00, -1.7977e+308,\n",
      "          -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -6.5318e+00,  -2.0846e+00,  -4.6459e+00,\n",
      "           -2.6484e+00, -1.7977e+308, -1.7977e+308, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -8.3169e+00,  -3.0402e+00,  -3.8697e+00,\n",
      "           -4.0482e+00,  -4.4336e+00,  -5.3755e+00, -1.7977e+308, -1.7977e+308]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -1.0628e+01,  -4.1267e+00,  -5.3515e+00,\n",
      "           -7.4565e+00,  -6.3595e+00,  -5.5184e+00,  -7.6868e+00,  -6.4620e+00]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -1.1845e+01,  -5.6171e+00,  -5.3431e+00,\n",
      "           -1.0518e+01,  -7.5759e+00,  -7.6319e+00,  -6.7348e+00,  -7.0088e+00]],\n",
      "\n",
      "        [[-1.7977e+308, -1.7977e+308,  -1.4099e+01,  -6.9403e+00,  -7.5979e+00,\n",
      "           -7.4515e+00,  -9.8307e+00,  -8.6438e+00,  -8.9896e+00,  -8.0580e+00]]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<CopySlices>)\n",
      "[zero_padding + target_lengths * 2 - 1, zero_padding + target_lengths * 2], [tensor([7], device='cuda:0'), tensor([8], device='cuda:0')]\n",
      "path.size, torch.Size([6, 1])\n",
      "path, tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]], device='cuda:0')\n",
      "l1l2.max(dim = -1).indices tensor([0], device='cuda:0')\n",
      "zero_padding + 2 * target_lengths - 1, tensor([7], device='cuda:0')\n",
      "path[input_lengths - 1, B], tensor([7], device='cuda:0')\n",
      "list(enumerate(path))[1:]), [(1, tensor([0], device='cuda:0')), (2, tensor([0], device='cuda:0')), (3, tensor([0], device='cuda:0')), (4, tensor([0], device='cuda:0')), (5, tensor([7], device='cuda:0'))]\n",
      "reversed(list(enumerate(path))[1:]) <list_reverseiterator object at 0x7eff2407db50>\n",
      "(t, indices)= 5 tensor([7], device='cuda:0')\n",
      "diff_labels[B, (indices - zero_padding).clamp(min = 0)], tensor([True], device='cuda:0')\n",
      "(indices - 2) * diff_labels[B, (indices - zero_padding).clamp(min = 0)] tensor([5], device='cuda:0')\n",
      "indices_ tensor([[5, 6, 7]], device='cuda:0')\n",
      "path tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [6],\n",
      "        [7]], device='cuda:0')\n",
      "(t, indices)= 4 tensor([6], device='cuda:0')\n",
      "diff_labels[B, (indices - zero_padding).clamp(min = 0)], tensor([False], device='cuda:0')\n",
      "(indices - 2) * diff_labels[B, (indices - zero_padding).clamp(min = 0)] tensor([0], device='cuda:0')\n",
      "indices_ tensor([[0, 5, 6]], device='cuda:0')\n",
      "path tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [6],\n",
      "        [6],\n",
      "        [7]], device='cuda:0')\n",
      "(t, indices)= 3 tensor([6], device='cuda:0')\n",
      "diff_labels[B, (indices - zero_padding).clamp(min = 0)], tensor([False], device='cuda:0')\n",
      "(indices - 2) * diff_labels[B, (indices - zero_padding).clamp(min = 0)] tensor([0], device='cuda:0')\n",
      "indices_ tensor([[0, 5, 6]], device='cuda:0')\n",
      "path tensor([[0],\n",
      "        [0],\n",
      "        [5],\n",
      "        [6],\n",
      "        [6],\n",
      "        [7]], device='cuda:0')\n",
      "(t, indices)= 2 tensor([5], device='cuda:0')\n",
      "diff_labels[B, (indices - zero_padding).clamp(min = 0)], tensor([True], device='cuda:0')\n",
      "(indices - 2) * diff_labels[B, (indices - zero_padding).clamp(min = 0)] tensor([3], device='cuda:0')\n",
      "indices_ tensor([[3, 4, 5]], device='cuda:0')\n",
      "path tensor([[0],\n",
      "        [3],\n",
      "        [5],\n",
      "        [6],\n",
      "        [6],\n",
      "        [7]], device='cuda:0')\n",
      "(t, indices)= 1 tensor([3], device='cuda:0')\n",
      "diff_labels[B, (indices - zero_padding).clamp(min = 0)], tensor([False], device='cuda:0')\n",
      "(indices - 2) * diff_labels[B, (indices - zero_padding).clamp(min = 0)] tensor([0], device='cuda:0')\n",
      "indices_ tensor([[0, 2, 3]], device='cuda:0')\n",
      "path tensor([[3],\n",
      "        [3],\n",
      "        [5],\n",
      "        [6],\n",
      "        [6],\n",
      "        [7]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test abc-2, max path\n",
    "\n",
    "probs_seq1 = [[\n",
    "            0.06390443, 0.42124858, 0.27323887, 0.06870235, 0.0361254,\n",
    "            0.18184413, 0.16493624\n",
    "        ], [\n",
    "            0.03309247, 0.42866108, 0.24390638, 0.09699597, 0.31895462,\n",
    "            0.0094893, 0.06890021\n",
    "        ], [\n",
    "            0.218104, 0.49992557, 0.18245131, 0.08503348, 0.14903535,\n",
    "            0.08424043, 0.08120984\n",
    "        ], [\n",
    "            0.12094152, 0.41162472, 0.01473646, 0.28045061, 0.24246305,\n",
    "            0.05206269, 0.09772094\n",
    "        ], [\n",
    "            0.5333387, 0.40550838, 0.00301669, 0.21745861, 0.20803985,\n",
    "            0.41317442, 0.01946335\n",
    "        ], [\n",
    "            0.16468227, 0.4180699, 0.1906545, 0.53963251, 0.19860937,\n",
    "            0.04377724, 0.01457421\n",
    "        ]]\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "torch.set_printoptions(precision=4)\n",
    "\n",
    "\n",
    "# import ctc\n",
    "\n",
    "T, B, C = 6, 1, 7\n",
    "t = 3\n",
    "blank = 0\n",
    "device = 'cuda'\n",
    "seed = 1\n",
    "atol = 1e-3\n",
    "for set_seed in [torch.manual_seed] + ([torch.cuda.manual_seed_all] if device == 'cuda' else []):\n",
    "\tset_seed(seed)\n",
    "tictoc = lambda: (device == 'cuda' and torch.cuda.synchronize()) or time.time()\n",
    "\n",
    "# logits = torch.randn(T, B, C, device = device).requires_grad_()\n",
    "import numpy\n",
    "logits = torch.reshape(torch.as_tensor(numpy.array(numpy.log(probs_seq1)),device = device), (6,1,7)).requires_grad_()\n",
    "\n",
    "# targets = torch.randint(blank + 1, C, (B, t), dtype = torch.long, device = device)\n",
    "targets = torch.reshape(torch.as_tensor(numpy.array([1,2,3]), dtype = torch.long, device = device), (B, t))\n",
    "\n",
    "\n",
    "\n",
    "input_lengths = torch.full((B,), T, dtype = torch.long, device = device)\n",
    "target_lengths = torch.full((B,), t, dtype = torch.long, device = device)\n",
    "log_probs = logits.log_softmax(dim = -1)\n",
    "print(\"log_probs\",log_probs)\n",
    "print('Device:', device)\n",
    "print('Log-probs shape (time X batch X channels):', 'x'.join(map(str, log_probs.shape)))\n",
    "\n",
    "tic = tictoc()\n",
    "builtin_ctc = F.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank = 0, reduction = 'none')\n",
    "toc = tictoc()\n",
    "builtin_ctc_grad, = torch.autograd.grad(builtin_ctc.sum(), logits, retain_graph = True)\n",
    "print('Built-in CTC loss', 'fwd', toc - tic, 'bwd', tictoc() - toc)\n",
    "\n",
    "tic = tictoc()\n",
    "custom_ctc = ctc_loss(log_probs, targets, input_lengths, target_lengths, blank = 0, reduction = 'none')\n",
    "toc = tictoc()\n",
    "custom_ctc_grad, = torch.autograd.grad(custom_ctc.sum(), logits, retain_graph = True)\n",
    "print('Custom CTC loss', 'fwd', toc - tic, 'bwd', tictoc() - toc)\n",
    "\n",
    "ce_alignment_targets = ctc_alignment_targets(log_probs, targets, input_lengths, target_lengths, blank = 0)\n",
    "ce_ctc = -ce_alignment_targets * log_probs\n",
    "ce_ctc_grad, = torch.autograd.grad(ce_ctc.sum(), logits, retain_graph = True)\n",
    "\n",
    "print('Custom loss matches:', torch.allclose(builtin_ctc, custom_ctc, atol = atol))\n",
    "print('Grad matches:', torch.allclose(builtin_ctc_grad, custom_ctc_grad, atol = atol))\n",
    "print('CE grad matches:', torch.allclose(builtin_ctc_grad, ce_ctc_grad, atol = atol))\n",
    "\n",
    "alignment = ctc_max_loss(log_probs, targets, input_lengths, target_lengths, blank = 0, reduction = 'none', alignment = True)\n",
    "a = alignment[:, 0, :target_lengths[0]]\n",
    "# print(a.t().cpu())\n",
    "plt.subplot(211)\n",
    "plt.title('Input-Output Viterbi alignment')\n",
    "plt.imshow(a.t().cpu(), origin = 'lower', aspect = 'auto')\n",
    "plt.xlabel('Input steps')\n",
    "plt.ylabel('Output steps')\n",
    "plt.subplot(212)\n",
    "plt.title('CTC alignment targets')\n",
    "a = ce_alignment_targets[:, 0, :]\n",
    "plt.imshow(a.t().cpu(), origin = 'lower', aspect = 'auto')\n",
    "plt.xlabel('Input steps')\n",
    "plt.ylabel(f'Output symbols, blank {blank}')\n",
    "plt.subplots_adjust(hspace = 0.5)\n",
    "plt.savefig('alignment.png')\n",
    "torch.set_printoptions(precision=2)\n",
    "# print(a.t().cpu())\n",
    "# print(targets[:,0])\n",
    "# print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([-0.6540, -0.1002,  0.3037,  1.6871]),\n",
       "indices=tensor([2, 0, 1, 1]))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(4, 4)\n",
    "torch.max(a, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_probs tensor([[[-2.94, -1.06, -1.49, -2.87, -3.51, -1.90, -1.99]],\n",
      "\n",
      "        [[-3.59, -1.03, -1.59, -2.52, -1.33, -4.84, -2.86]],\n",
      "\n",
      "        [[-1.79, -0.96, -1.96, -2.73, -2.17, -2.74, -2.77]],\n",
      "\n",
      "        [[-2.31, -1.09, -4.42, -1.47, -1.62, -3.15, -2.52]],\n",
      "\n",
      "        [[-0.86, -1.69, -6.59, -2.31, -2.36, -1.67, -4.73]],\n",
      "\n",
      "        [[-2.25, -1.32, -2.11, -1.07, -2.07, -3.58, -4.68]]], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<LogSoftmaxBackward>)\n",
      "Device: cuda\n",
      "Log-probs shape (time X batch X channels): 6x1x7\n",
      "Built-in CTC loss fwd 0.0004189014434814453 bwd 0.0009527206420898438\n",
      "B, tensor([0], device='cuda:0')\n",
      "targets, tensor([[1, 2, 3]], device='cuda:0')\n",
      "targets_, tensor([[1, 2, 3, 1]], device='cuda:0')\n",
      "targets_, tensor([[0, 1, 0, 2, 0, 3, 0, 1]], device='cuda:0')\n",
      "targets_[:, 2:] != targets_[:, :-2], tensor([[False,  True, False,  True, False,  True]], device='cuda:0')\n",
      "targets_[:, 2:], tensor([[0, 2, 0, 3, 0, 1]], device='cuda:0')\n",
      "targets_[:, :-2], tensor([[0, 1, 0, 2, 0, 3]], device='cuda:0')\n",
      "diff_labels, tensor([[False, False, False,  True, False,  True, False,  True]],\n",
      "       device='cuda:0')\n",
      "zero, -1.7976931348623157e+308\n",
      "log_probs, tensor([[[-2.94, -1.06, -1.49, -2.87, -3.51, -1.90, -1.99]],\n",
      "\n",
      "        [[-3.59, -1.03, -1.59, -2.52, -1.33, -4.84, -2.86]],\n",
      "\n",
      "        [[-1.79, -0.96, -1.96, -2.73, -2.17, -2.74, -2.77]],\n",
      "\n",
      "        [[-2.31, -1.09, -4.42, -1.47, -1.62, -3.15, -2.52]],\n",
      "\n",
      "        [[-0.86, -1.69, -6.59, -2.31, -2.36, -1.67, -4.73]],\n",
      "\n",
      "        [[-2.25, -1.32, -2.11, -1.07, -2.07, -3.58, -4.68]]], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<LogSoftmaxBackward>)\n",
      "targets_.expand(len(log_probs), -1, -1), tensor([[[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]]], device='cuda:0')\n",
      "log_probs_, tensor([[[-2.94, -1.06, -2.94, -1.49, -2.94, -2.87, -2.94, -1.06]],\n",
      "\n",
      "        [[-3.59, -1.03, -3.59, -1.59, -3.59, -2.52, -3.59, -1.03]],\n",
      "\n",
      "        [[-1.79, -0.96, -1.79, -1.96, -1.79, -2.73, -1.79, -0.96]],\n",
      "\n",
      "        [[-2.31, -1.09, -2.31, -4.42, -2.31, -1.47, -2.31, -1.09]],\n",
      "\n",
      "        [[-0.86, -1.69, -0.86, -6.59, -0.86, -2.31, -0.86, -1.69]],\n",
      "\n",
      "        [[-2.25, -1.32, -2.25, -2.11, -2.25, -1.07, -2.25, -1.32]]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<GatherBackward>)\n",
      "log_probs[0, :, blank], tensor([-2.94], device='cuda:0', dtype=torch.float64, grad_fn=<SelectBackward>)\n",
      "log_probs[0, B, targets_[:, 1]], tensor([-1.06], device='cuda:0', dtype=torch.float64, grad_fn=<IndexBackward>)\n",
      "log_alpha.size(), torch.Size([6, 1, 10])\n",
      "log_alpha, tensor([[[-1.80e+308, -1.80e+308,  -2.94e+00,  -1.06e+00, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308]]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<CopySlices>)\n",
      "torch.stack tensor([[[ -2.94e+00,  -1.06e+00, -1.80e+308, -1.80e+308, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308,  -2.94e+00,  -1.06e+00, -1.80e+308, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308, -1.80e+308,  -1.06e+00, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308]]], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<StackBackward>)\n",
      "torch.logsumexp(aa, dim = 0) tensor([[ -2.94e+00,  -9.14e-01,  -1.06e+00,  -1.06e+00, -1.80e+308, -1.80e+308,\n",
      "         -1.80e+308, -1.80e+308]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LogsumexpBackward>)\n",
      "log_alpha, tensor([[[-1.80e+308, -1.80e+308,  -2.94e+00,  -1.06e+00, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308,  -6.53e+00,  -1.94e+00,  -4.65e+00,\n",
      "           -2.65e+00, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308,  -8.32e+00,  -2.89e+00,  -3.66e+00,\n",
      "           -3.46e+00,  -4.43e+00,  -5.38e+00, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308,  -1.06e+01,  -3.97e+00,  -4.82e+00,\n",
      "           -6.60e+00,  -5.45e+00,  -4.51e+00,  -7.69e+00,  -6.46e+00]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308,  -1.15e+01,  -5.66e+00,  -4.47e+00,\n",
      "           -1.02e+01,  -6.03e+00,  -6.41e+00,  -5.33e+00,  -6.03e+00]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308,  -1.37e+01,  -6.98e+00,  -6.46e+00,\n",
      "           -6.31e+00,  -8.27e+00,  -6.57e+00,  -7.29e+00,  -6.04e+00]]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<CopySlices>)\n",
      "[zero_padding + target_lengths * 2 - 1, zero_padding + target_lengths * 2], [tensor([7], device='cuda:0'), tensor([8], device='cuda:0')]\n",
      "Custom CTC loss fwd 0.019848108291625977 bwd 0.007051229476928711\n",
      "Custom loss matches: True\n",
      "Grad matches: True\n",
      "CE grad matches: True\n",
      "B, tensor([0], device='cuda:0')\n",
      "targets, tensor([[1, 2, 3]], device='cuda:0')\n",
      "targets_, tensor([[1, 2, 3, 1]], device='cuda:0')\n",
      "targets_, tensor([[0, 1, 0, 2, 0, 3, 0, 1]], device='cuda:0')\n",
      "targets_[:, 2:] != targets_[:, :-2], tensor([[False,  True, False,  True, False,  True]], device='cuda:0')\n",
      "targets_[:, 2:], tensor([[0, 2, 0, 3, 0, 1]], device='cuda:0')\n",
      "targets_[:, :-2], tensor([[0, 1, 0, 2, 0, 3]], device='cuda:0')\n",
      "diff_labels, tensor([[False, False, False,  True, False,  True, False,  True]],\n",
      "       device='cuda:0')\n",
      "zero, -1.7976931348623157e+308\n",
      "log_probs, tensor([[[-2.94, -1.06, -1.49, -2.87, -3.51, -1.90, -1.99]],\n",
      "\n",
      "        [[-3.59, -1.03, -1.59, -2.52, -1.33, -4.84, -2.86]],\n",
      "\n",
      "        [[-1.79, -0.96, -1.96, -2.73, -2.17, -2.74, -2.77]],\n",
      "\n",
      "        [[-2.31, -1.09, -4.42, -1.47, -1.62, -3.15, -2.52]],\n",
      "\n",
      "        [[-0.86, -1.69, -6.59, -2.31, -2.36, -1.67, -4.73]],\n",
      "\n",
      "        [[-2.25, -1.32, -2.11, -1.07, -2.07, -3.58, -4.68]]], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<LogSoftmaxBackward>)\n",
      "targets_.expand(len(log_probs), -1, -1), tensor([[[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]]], device='cuda:0')\n",
      "log_probs_, tensor([[[-2.94, -1.06, -2.94, -1.49, -2.94, -2.87, -2.94, -1.06]],\n",
      "\n",
      "        [[-3.59, -1.03, -3.59, -1.59, -3.59, -2.52, -3.59, -1.03]],\n",
      "\n",
      "        [[-1.79, -0.96, -1.79, -1.96, -1.79, -2.73, -1.79, -0.96]],\n",
      "\n",
      "        [[-2.31, -1.09, -2.31, -4.42, -2.31, -1.47, -2.31, -1.09]],\n",
      "\n",
      "        [[-0.86, -1.69, -0.86, -6.59, -0.86, -2.31, -0.86, -1.69]],\n",
      "\n",
      "        [[-2.25, -1.32, -2.25, -2.11, -2.25, -1.07, -2.25, -1.32]]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<GatherBackward>)\n",
      "log_probs[0, :, blank], tensor([-2.94], device='cuda:0', dtype=torch.float64, grad_fn=<SelectBackward>)\n",
      "log_probs[0, B, targets_[:, 1]], tensor([-1.06], device='cuda:0', dtype=torch.float64, grad_fn=<IndexBackward>)\n",
      "log_alpha.size(), torch.Size([6, 1, 10])\n",
      "log_alpha, tensor([[[-1.80e+308, -1.80e+308,  -2.94e+00,  -1.06e+00, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308]]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<CopySlices>)\n",
      "torch.stack tensor([[[ -2.94e+00,  -1.06e+00, -1.80e+308, -1.80e+308, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308,  -2.94e+00,  -1.06e+00, -1.80e+308, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308, -1.80e+308,  -1.06e+00, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308]]], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<StackBackward>)\n",
      "torch.logsumexp(aa, dim = 0) tensor([[ -2.94e+00,  -9.14e-01,  -1.06e+00,  -1.06e+00, -1.80e+308, -1.80e+308,\n",
      "         -1.80e+308, -1.80e+308]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LogsumexpBackward>)\n",
      "log_alpha, tensor([[[-1.80e+308, -1.80e+308,  -2.94e+00,  -1.06e+00, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308,  -6.53e+00,  -1.94e+00,  -4.65e+00,\n",
      "           -2.65e+00, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308,  -8.32e+00,  -2.89e+00,  -3.66e+00,\n",
      "           -3.46e+00,  -4.43e+00,  -5.38e+00, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308,  -1.06e+01,  -3.97e+00,  -4.82e+00,\n",
      "           -6.60e+00,  -5.45e+00,  -4.51e+00,  -7.69e+00,  -6.46e+00]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308,  -1.15e+01,  -5.66e+00,  -4.47e+00,\n",
      "           -1.02e+01,  -6.03e+00,  -6.41e+00,  -5.33e+00,  -6.03e+00]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308,  -1.37e+01,  -6.98e+00,  -6.46e+00,\n",
      "           -6.31e+00,  -8.27e+00,  -6.57e+00,  -7.29e+00,  -6.04e+00]]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<CopySlices>)\n",
      "[zero_padding + target_lengths * 2 - 1, zero_padding + target_lengths * 2], [tensor([7], device='cuda:0'), tensor([8], device='cuda:0')]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test abc-3\n",
    "\n",
    "probs_seq1 = [[\n",
    "            0.06390443, 0.42124858, 0.27323887, 0.06870235, 0.0361254,\n",
    "            0.18184413, 0.16493624\n",
    "        ], [\n",
    "            0.03309247, 0.42866108, 0.24390638, 0.09699597, 0.31895462,\n",
    "            0.0094893, 0.06890021\n",
    "        ], [\n",
    "            0.218104, 0.49992557, 0.18245131, 0.08503348, 0.14903535,\n",
    "            0.08424043, 0.08120984\n",
    "        ], [\n",
    "            0.12094152, 0.41162472, 0.01473646, 0.28045061, 0.24246305,\n",
    "            0.05206269, 0.09772094\n",
    "        ], [\n",
    "            0.9333387, 0.40550838, 0.00301669, 0.21745861, 0.20803985,\n",
    "            0.41317442, 0.01946335\n",
    "        ], [\n",
    "            0.16468227, 0.4180699, 0.1906545, 0.53963251, 0.19860937,\n",
    "            0.04377724, 0.01457421\n",
    "        ]]\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "torch.set_printoptions(precision=2)\n",
    "\n",
    "\n",
    "# import ctc\n",
    "\n",
    "T, B, C = 6, 1, 7\n",
    "t = 3\n",
    "blank = 0\n",
    "device = 'cuda'\n",
    "seed = 1\n",
    "atol = 1e-3\n",
    "for set_seed in [torch.manual_seed] + ([torch.cuda.manual_seed_all] if device == 'cuda' else []):\n",
    "\tset_seed(seed)\n",
    "tictoc = lambda: (device == 'cuda' and torch.cuda.synchronize()) or time.time()\n",
    "\n",
    "# logits = torch.randn(T, B, C, device = device).requires_grad_()\n",
    "import numpy\n",
    "logits = torch.reshape(torch.as_tensor(numpy.array(numpy.log(probs_seq1)),device = device), (6,1,7)).requires_grad_()\n",
    "\n",
    "# targets = torch.randint(blank + 1, C, (B, t), dtype = torch.long, device = device)\n",
    "targets = torch.reshape(torch.as_tensor(numpy.array([1,2,3]), dtype = torch.long, device = device), (B, t))\n",
    "\n",
    "\n",
    "\n",
    "input_lengths = torch.full((B,), T, dtype = torch.long, device = device)\n",
    "target_lengths = torch.full((B,), t, dtype = torch.long, device = device)\n",
    "log_probs = logits.log_softmax(dim = -1)\n",
    "print(\"log_probs\",log_probs)\n",
    "print('Device:', device)\n",
    "print('Log-probs shape (time X batch X channels):', 'x'.join(map(str, log_probs.shape)))\n",
    "\n",
    "tic = tictoc()\n",
    "builtin_ctc = F.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank = 0, reduction = 'none')\n",
    "toc = tictoc()\n",
    "builtin_ctc_grad, = torch.autograd.grad(builtin_ctc.sum(), logits, retain_graph = True)\n",
    "print('Built-in CTC loss', 'fwd', toc - tic, 'bwd', tictoc() - toc)\n",
    "\n",
    "tic = tictoc()\n",
    "custom_ctc = ctc_loss(log_probs, targets, input_lengths, target_lengths, blank = 0, reduction = 'none')\n",
    "toc = tictoc()\n",
    "custom_ctc_grad, = torch.autograd.grad(custom_ctc.sum(), logits, retain_graph = True)\n",
    "print('Custom CTC loss', 'fwd', toc - tic, 'bwd', tictoc() - toc)\n",
    "\n",
    "ce_alignment_targets = ctc_alignment_targets(log_probs, targets, input_lengths, target_lengths, blank = 0)\n",
    "ce_ctc = -ce_alignment_targets * log_probs\n",
    "ce_ctc_grad, = torch.autograd.grad(ce_ctc.sum(), logits, retain_graph = True)\n",
    "\n",
    "print('Custom loss matches:', torch.allclose(builtin_ctc, custom_ctc, atol = atol))\n",
    "print('Grad matches:', torch.allclose(builtin_ctc_grad, custom_ctc_grad, atol = atol))\n",
    "print('CE grad matches:', torch.allclose(builtin_ctc_grad, ce_ctc_grad, atol = atol))\n",
    "\n",
    "alignment = ctc_loss(log_probs, targets, input_lengths, target_lengths, blank = 0, reduction = 'none', alignment = True)\n",
    "a = alignment[:, 0, :target_lengths[0]]\n",
    "# print(a.t().cpu())\n",
    "plt.subplot(211)\n",
    "plt.title('Input-Output Viterbi alignment')\n",
    "plt.imshow(a.t().cpu(), origin = 'lower', aspect = 'auto')\n",
    "plt.xlabel('Input steps')\n",
    "plt.ylabel('Output steps')\n",
    "plt.subplot(212)\n",
    "plt.title('CTC alignment targets')\n",
    "a = ce_alignment_targets[:, 0, :]\n",
    "plt.imshow(a.t().cpu(), origin = 'lower', aspect = 'auto')\n",
    "plt.xlabel('Input steps')\n",
    "plt.ylabel(f'Output symbols, blank {blank}')\n",
    "plt.subplots_adjust(hspace = 0.5)\n",
    "plt.savefig('alignment.png')\n",
    "torch.set_printoptions(precision=2)\n",
    "# print(a.t().cpu())\n",
    "# print(targets[:,0])\n",
    "# print(logits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.095561027527])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.109861228867e+01, -1.799999989014e+08]], dtype=torch.float64)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_printoptions(precision=12)\n",
    "print(torch.log(torch.as_tensor([2.71*3])))\n",
    "a=torch.as_tensor([[[1e1, -1.80e+8,]],[[1e1, -1.80e+8,]],[[1e1, -1.80e+8,]]], dtype=torch.float64)\n",
    "torch.logsumexp(a, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3]], device='cuda:0') tensor([[1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(targets, targets[:, :1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([[1,2],[3,4]])\n",
    "r = torch.gather(t, 1, torch.tensor([[0,0],[1,0]]))\n",
    "index = [[0,0],[1,0]]\n",
    "i,j,k = 0,0,1\n",
    "print(t[index[j][k]][k] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1],\n",
      "        [4, 3]])\n"
     ]
    }
   ],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1],\n",
       "        [4, 3]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.gather(1, torch.tensor([[0,0],[1,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1],\n",
       "        [4, 3]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.gather(-1, torch.tensor([[0,0],[1,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_probs tensor([[[-2.94, -1.06, -1.49, -2.87, -3.51, -1.90, -1.99]],\n",
      "\n",
      "        [[-3.59, -1.03, -1.59, -2.52, -1.33, -4.84, -2.86]],\n",
      "\n",
      "        [[-1.79, -0.96, -1.96, -2.73, -2.17, -2.74, -2.77]]], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<LogSoftmaxBackward>)\n",
      "Device: cuda\n",
      "Log-probs shape (time X batch X channels): 3x1x7\n",
      "Built-in CTC loss fwd 0.00092315673828125 bwd 0.0007777214050292969\n",
      "B, tensor([0], device='cuda:0')\n",
      "targets, tensor([[1, 2]], device='cuda:0')\n",
      "targets_, tensor([[1, 2, 1]], device='cuda:0')\n",
      "targets_, tensor([[0, 1, 0, 2, 0, 1]], device='cuda:0')\n",
      "targets_[:, 2:] != targets_[:, :-2], tensor([[False,  True, False,  True]], device='cuda:0')\n",
      "targets_[:, 2:], tensor([[0, 2, 0, 1]], device='cuda:0')\n",
      "targets_[:, :-2], tensor([[0, 1, 0, 2]], device='cuda:0')\n",
      "diff_labels, tensor([[False, False, False,  True, False,  True]], device='cuda:0')\n",
      "zero, -1.7976931348623157e+308\n",
      "log_probs, tensor([[[-2.94, -1.06, -1.49, -2.87, -3.51, -1.90, -1.99]],\n",
      "\n",
      "        [[-3.59, -1.03, -1.59, -2.52, -1.33, -4.84, -2.86]],\n",
      "\n",
      "        [[-1.79, -0.96, -1.96, -2.73, -2.17, -2.74, -2.77]]], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<LogSoftmaxBackward>)\n",
      "targets_.expand(len(log_probs), -1, -1), tensor([[[0, 1, 0, 2, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 1]]], device='cuda:0')\n",
      "log_probs_, tensor([[[-2.94, -1.06, -2.94, -1.49, -2.94, -1.06]],\n",
      "\n",
      "        [[-3.59, -1.03, -3.59, -1.59, -3.59, -1.03]],\n",
      "\n",
      "        [[-1.79, -0.96, -1.79, -1.96, -1.79, -0.96]]], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<GatherBackward>)\n",
      "log_probs[0, :, blank], tensor([-2.94], device='cuda:0', dtype=torch.float64, grad_fn=<SelectBackward>)\n",
      "log_probs[0, B, targets_[:, 1]], tensor([-1.06], device='cuda:0', dtype=torch.float64, grad_fn=<IndexBackward>)\n",
      "log_alpha.size(), torch.Size([3, 1, 8])\n",
      "log_alpha, tensor([[[-1.80e+308, -1.80e+308,  -2.94e+00,  -1.06e+00, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308]]], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CopySlices>)\n",
      "torch.stack tensor([[[ -2.94e+00,  -1.06e+00, -1.80e+308, -1.80e+308, -1.80e+308,\n",
      "          -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308,  -2.94e+00,  -1.06e+00, -1.80e+308, -1.80e+308,\n",
      "          -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308, -1.80e+308,  -1.06e+00, -1.80e+308,\n",
      "          -1.80e+308]]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<StackBackward>)\n",
      "torch.logsumexp(aa, dim = 0) tensor([[ -2.94e+00,  -9.14e-01,  -1.06e+00,  -1.06e+00, -1.80e+308, -1.80e+308]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<LogsumexpBackward>)\n",
      "log_alpha, tensor([[[-1.80e+308, -1.80e+308,  -2.94e+00,  -1.06e+00, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308,  -6.53e+00,  -1.94e+00,  -4.65e+00,\n",
      "           -2.65e+00, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308,  -8.32e+00,  -2.89e+00,  -3.66e+00,\n",
      "           -3.46e+00,  -4.43e+00,  -3.60e+00]]], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CopySlices>)\n",
      "[zero_padding + target_lengths * 2 - 1, zero_padding + target_lengths * 2], [tensor([5], device='cuda:0'), tensor([6], device='cuda:0')]\n",
      "Custom CTC loss fwd 0.019101619720458984 bwd 0.004287242889404297\n",
      "Custom loss matches: True\n",
      "Grad matches: True\n",
      "CE grad matches: True\n",
      "B, tensor([0], device='cuda:0')\n",
      "targets, tensor([[1, 2]], device='cuda:0')\n",
      "targets_, tensor([[1, 2, 1]], device='cuda:0')\n",
      "targets_, tensor([[0, 1, 0, 2, 0, 1]], device='cuda:0')\n",
      "targets_[:, 2:] != targets_[:, :-2], tensor([[False,  True, False,  True]], device='cuda:0')\n",
      "targets_[:, 2:], tensor([[0, 2, 0, 1]], device='cuda:0')\n",
      "targets_[:, :-2], tensor([[0, 1, 0, 2]], device='cuda:0')\n",
      "diff_labels, tensor([[False, False, False,  True, False,  True]], device='cuda:0')\n",
      "zero, -1.7976931348623157e+308\n",
      "log_probs, tensor([[[-2.94, -1.06, -1.49, -2.87, -3.51, -1.90, -1.99]],\n",
      "\n",
      "        [[-3.59, -1.03, -1.59, -2.52, -1.33, -4.84, -2.86]],\n",
      "\n",
      "        [[-1.79, -0.96, -1.96, -2.73, -2.17, -2.74, -2.77]]], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<LogSoftmaxBackward>)\n",
      "targets_.expand(len(log_probs), -1, -1), tensor([[[0, 1, 0, 2, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 1]]], device='cuda:0')\n",
      "log_probs_, tensor([[[-2.94, -1.06, -2.94, -1.49, -2.94, -1.06]],\n",
      "\n",
      "        [[-3.59, -1.03, -3.59, -1.59, -3.59, -1.03]],\n",
      "\n",
      "        [[-1.79, -0.96, -1.79, -1.96, -1.79, -0.96]]], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<GatherBackward>)\n",
      "log_probs[0, :, blank], tensor([-2.94], device='cuda:0', dtype=torch.float64, grad_fn=<SelectBackward>)\n",
      "log_probs[0, B, targets_[:, 1]], tensor([-1.06], device='cuda:0', dtype=torch.float64, grad_fn=<IndexBackward>)\n",
      "log_alpha.size(), torch.Size([3, 1, 8])\n",
      "log_alpha, tensor([[[-1.80e+308, -1.80e+308,  -2.94e+00,  -1.06e+00, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308]]], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CopySlices>)\n",
      "torch.stack tensor([[[ -2.94e+00,  -1.06e+00, -1.80e+308, -1.80e+308, -1.80e+308,\n",
      "          -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308,  -2.94e+00,  -1.06e+00, -1.80e+308, -1.80e+308,\n",
      "          -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308, -1.80e+308,  -1.06e+00, -1.80e+308,\n",
      "          -1.80e+308]]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<StackBackward>)\n",
      "torch.logsumexp(aa, dim = 0) tensor([[ -2.94e+00,  -9.14e-01,  -1.06e+00,  -1.06e+00, -1.80e+308, -1.80e+308]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<LogsumexpBackward>)\n",
      "log_alpha, tensor([[[-1.80e+308, -1.80e+308,  -2.94e+00,  -1.06e+00, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308,  -6.53e+00,  -1.94e+00,  -4.65e+00,\n",
      "           -2.65e+00, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308,  -8.32e+00,  -2.89e+00,  -3.66e+00,\n",
      "           -3.46e+00,  -4.43e+00,  -3.60e+00]]], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CopySlices>)\n",
      "[zero_padding + target_lengths * 2 - 1, zero_padding + target_lengths * 2], [tensor([5], device='cuda:0'), tensor([6], device='cuda:0')]\n",
      "path.size, torch.Size([3, 1])\n",
      "path, tensor([[0],\n",
      "        [0],\n",
      "        [0]], device='cuda:0')\n",
      "l1l2.max(dim = -1).indices tensor([0], device='cuda:0')\n",
      "zero_padding + 2 * target_lengths - 1, tensor([5], device='cuda:0')\n",
      "path[input_lengths - 1, B], tensor([5], device='cuda:0')\n",
      "list(enumerate(path))[1:]), [(1, tensor([0], device='cuda:0')), (2, tensor([5], device='cuda:0'))]\n",
      "reversed(list(enumerate(path))[1:]) <list_reverseiterator object at 0x7eff705bda50>\n",
      "(t, indices)= 2 tensor([5], device='cuda:0')\n",
      "diff_labels[B, (indices - zero_padding).clamp(min = 0)], tensor([True], device='cuda:0')\n",
      "(indices - 2) * diff_labels[B, (indices - zero_padding).clamp(min = 0)] tensor([3], device='cuda:0')\n",
      "indices_ tensor([[3, 4, 5]], device='cuda:0')\n",
      "path tensor([[0],\n",
      "        [3],\n",
      "        [5]], device='cuda:0')\n",
      "(t, indices)= 1 tensor([3], device='cuda:0')\n",
      "diff_labels[B, (indices - zero_padding).clamp(min = 0)], tensor([False], device='cuda:0')\n",
      "(indices - 2) * diff_labels[B, (indices - zero_padding).clamp(min = 0)] tensor([0], device='cuda:0')\n",
      "indices_ tensor([[0, 2, 3]], device='cuda:0')\n",
      "path tensor([[3],\n",
      "        [3],\n",
      "        [5]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test abc-3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "probs_seq1 = [[\n",
    "            0.06390443, 0.42124858, 0.27323887, 0.06870235, 0.0361254,\n",
    "            0.18184413, 0.16493624\n",
    "        ], [\n",
    "            0.03309247, 0.42866108, 0.24390638, 0.09699597, 0.31895462,\n",
    "            0.0094893, 0.06890021\n",
    "        ], [\n",
    "            0.218104, 0.49992557, 0.18245131, 0.08503348, 0.14903535,\n",
    "            0.08424043, 0.08120984\n",
    "        ], [\n",
    "            0.12094152, 0.41162472, 0.01473646, 0.28045061, 0.24246305,\n",
    "            0.05206269, 0.09772094\n",
    "        ], [\n",
    "            0.9333387, 0.40550838, 0.00301669, 0.21745861, 0.20803985,\n",
    "            0.41317442, 0.01946335\n",
    "        ], [\n",
    "            0.16468227, 0.4180699, 0.1906545, 0.53963251, 0.19860937,\n",
    "            0.04377724, 0.01457421\n",
    "        ]]\n",
    "\n",
    "probs_seq2 = [[\n",
    "            0.06390443, 0.42124858, 0.27323887, 0.06870235, 0.0361254,\n",
    "            0.18184413, 0.16493624\n",
    "        ], [\n",
    "            0.03309247, 0.42866108, 0.24390638, 0.09699597, 0.31895462,\n",
    "            0.0094893, 0.06890021\n",
    "        ], [\n",
    "            0.218104, 0.49992557, 0.18245131, 0.08503348, 0.14903535,\n",
    "            0.08424043, 0.08120984\n",
    "        ]]\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "torch.set_printoptions(precision=2)\n",
    "\n",
    "\n",
    "# import ctc\n",
    "\n",
    "T, B, C = 3, 1, 7\n",
    "t = 2\n",
    "blank = 0\n",
    "device = 'cuda'\n",
    "seed = 1\n",
    "atol = 1e-3\n",
    "for set_seed in [torch.manual_seed] + ([torch.cuda.manual_seed_all] if device == 'cuda' else []):\n",
    "\tset_seed(seed)\n",
    "tictoc = lambda: (device == 'cuda' and torch.cuda.synchronize()) or time.time()\n",
    "\n",
    "# logits = torch.randn(T, B, C, device = device).requires_grad_()\n",
    "import numpy\n",
    "logits = torch.reshape(torch.as_tensor(numpy.array(numpy.log(probs_seq2)),device = device), (T,B,C)).requires_grad_()\n",
    "\n",
    "# targets = torch.randint(blank + 1, C, (B, t), dtype = torch.long, device = device)\n",
    "targets = torch.reshape(torch.as_tensor(numpy.array([1,2]), dtype = torch.long, device = device), (B, t))\n",
    "\n",
    "\n",
    "\n",
    "input_lengths = torch.full((B,), T, dtype = torch.long, device = device)\n",
    "target_lengths = torch.full((B,), t, dtype = torch.long, device = device)\n",
    "log_probs = logits.log_softmax(dim = -1)\n",
    "print(\"log_probs\",log_probs)\n",
    "print('Device:', device)\n",
    "print('Log-probs shape (time X batch X channels):', 'x'.join(map(str, log_probs.shape)))\n",
    "\n",
    "tic = tictoc()\n",
    "builtin_ctc = F.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank = 0, reduction = 'none')\n",
    "toc = tictoc()\n",
    "builtin_ctc_grad, = torch.autograd.grad(builtin_ctc.sum(), logits, retain_graph = True)\n",
    "print('Built-in CTC loss', 'fwd', toc - tic, 'bwd', tictoc() - toc)\n",
    "\n",
    "tic = tictoc()\n",
    "custom_ctc = ctc_loss(log_probs, targets, input_lengths, target_lengths, blank = 0, reduction = 'none')\n",
    "toc = tictoc()\n",
    "custom_ctc_grad, = torch.autograd.grad(custom_ctc.sum(), logits, retain_graph = True)\n",
    "print('Custom CTC loss', 'fwd', toc - tic, 'bwd', tictoc() - toc)\n",
    "\n",
    "ce_alignment_targets = ctc_alignment_targets(log_probs, targets, input_lengths, target_lengths, blank = 0)\n",
    "ce_ctc = -ce_alignment_targets * log_probs\n",
    "ce_ctc_grad, = torch.autograd.grad(ce_ctc.sum(), logits, retain_graph = True)\n",
    "\n",
    "print('Custom loss matches:', torch.allclose(builtin_ctc, custom_ctc, atol = atol))\n",
    "print('Grad matches:', torch.allclose(builtin_ctc_grad, custom_ctc_grad, atol = atol))\n",
    "print('CE grad matches:', torch.allclose(builtin_ctc_grad, ce_ctc_grad, atol = atol))\n",
    "\n",
    "alignment = ctc_loss(log_probs, targets, input_lengths, target_lengths, blank = 0, reduction = 'none', alignment = True)\n",
    "a = alignment[:, 0, :target_lengths[0]]\n",
    "# print(a.t().cpu())\n",
    "plt.subplot(211)\n",
    "plt.title('Input-Output Viterbi alignment')\n",
    "plt.imshow(a.t().cpu(), origin = 'lower', aspect = 'auto')\n",
    "plt.xlabel('Input steps')\n",
    "plt.ylabel('Output steps')\n",
    "plt.subplot(212)\n",
    "plt.title('CTC alignment targets')\n",
    "a = ce_alignment_targets[:, 0, :]\n",
    "plt.imshow(a.t().cpu(), origin = 'lower', aspect = 'auto')\n",
    "plt.xlabel('Input steps')\n",
    "plt.ylabel(f'Output symbols, blank {blank}')\n",
    "plt.subplots_adjust(hspace = 0.5)\n",
    "plt.savefig('alignment.png')\n",
    "torch.set_printoptions(precision=2)\n",
    "# print(a.t().cpu())\n",
    "# print(targets[:,0])\n",
    "# print(logits)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
