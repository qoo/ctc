{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#@torch.jit.script\n",
    "def ctc_loss(log_probs, targets, input_lengths, target_lengths, blank : int = 0, reduction : str = 'none', alignment : bool = False):\n",
    "\tB = torch.arange(len(targets), device = input_lengths.device)\n",
    "\tprint(\"B,\",B)\n",
    "\tprint(\"targets,\",targets)\n",
    "\ttargets_ = torch.cat([targets, targets[:, :1]], dim = -1)\n",
    "\tprint(\"targets_,\",targets_)\n",
    "\ttargets_ = torch.stack([torch.full_like(targets_, blank), targets_], dim = -1).flatten(start_dim = -2)\n",
    "\tprint(\"targets_,\",targets_)\n",
    "\tprint(\"targets_[:, 2:] != targets_[:, :-2],\", targets_[:, 2:] != targets_[:, :-2])\n",
    "\tprint(\"targets_[:, 2:],\",targets_[:, 2:])\n",
    "\tprint(\"targets_[:, :-2],\",targets_[:, :-2])\n",
    "\tdiff_labels = torch.cat([torch.as_tensor([[False, False]], device = targets.device).expand(len(B), -1), targets_[:, 2:] != targets_[:, :-2]], dim = 1)\n",
    "\tprint(\"diff_labels,\",diff_labels)\n",
    "\t# if the -inf is used as neutral element, custom logsumexp must be used\n",
    "\t#zero = float('-inf')\n",
    "\t# to avoid nan grad in torch.logsumexp\n",
    "\tzero = torch.finfo(log_probs.dtype).min\n",
    "\tprint(\"zero,\",zero)\n",
    "\n",
    "\tzero, zero_padding = torch.tensor(zero, device = log_probs.device, dtype = log_probs.dtype), 2\n",
    "\tprint(\"log_probs,\",log_probs)\n",
    "\tlog_probs_ = log_probs.gather(-1, targets_.expand(len(log_probs), -1, -1))\n",
    "\tprint(\"targets_.expand(len(log_probs), -1, -1),\",targets_.expand(len(log_probs), -1, -1))\n",
    "\tprint(\"log_probs_,\",log_probs_)\n",
    "\tlog_alpha = torch.full((len(log_probs), len(B), zero_padding + targets_.shape[-1]), zero, device = log_probs.device, dtype = log_probs.dtype)\n",
    "\tlog_alpha[0, :, zero_padding + 0] = log_probs[0, :, blank]\n",
    "\tprint(\"log_probs[0, :, blank],\",log_probs[0, :, blank])\n",
    "\tlog_alpha[0, :, zero_padding + 1] = log_probs[0, B, targets_[:, 1]]\n",
    "\tprint(\"log_alpha.size(),\",log_alpha.size())\n",
    "\tprint(\"log_alpha,\",log_alpha)\n",
    "\t# log_alpha[1:, :, zero_padding:] = log_probs.gather(-1, targets_.expand(len(log_probs), -1, -1))[1:]\n",
    "\tfor t in range(1, len(log_probs)):\n",
    "\t\tlog_alpha[t, :, 2:] = log_probs_[t] + logadd(log_alpha[t - 1, :, 2:], log_alpha[t - 1, :, 1:-1], torch.where(diff_labels, log_alpha[t - 1, :, :-2], zero))\n",
    "\tprint(\"log_alpha,\",log_alpha)\n",
    "\tl1l2 = log_alpha[input_lengths - 1, B].gather(-1, torch.stack([zero_padding + target_lengths * 2 - 1, zero_padding + target_lengths * 2], dim = -1)) \n",
    "\tloss = -torch.logsumexp(l1l2, dim = -1)\n",
    "\tif not alignment:\n",
    "\t\treturn loss\n",
    "\t\n",
    "\tpath = torch.zeros(len(log_alpha), len(B), device = log_alpha.device, dtype = torch.int64)\n",
    "\tpath[input_lengths - 1, B] = zero_padding + 2 * target_lengths - 1 + l1l2.max(dim = -1).indices\n",
    "\tfor t, indices in reversed(list(enumerate(path))[1:]):\n",
    "\t\tindices_ = torch.stack([(indices - 2) * diff_labels[B, (indices - zero_padding).clamp(min = 0)], (indices - 1).clamp(min = 0), indices], dim = -1)\n",
    "\t\tpath[t - 1] += (indices - 2 + log_alpha[t - 1, B].gather(-1, indices_).max(dim = -1).indices).clamp(min = 0)\n",
    "\treturn torch.zeros_like(log_alpha).scatter_(-1, path.unsqueeze(-1), 1.0)[..., (zero_padding + 1)::2]\n",
    "\n",
    "def ctc_alignment_targets(log_probs, targets, input_lengths, target_lengths, blank = 0, ctc_loss = F.ctc_loss, retain_graph = True):\n",
    "\tloss = ctc_loss(log_probs, targets, input_lengths, target_lengths, blank = blank, reduction = 'sum')\n",
    "\tprobs = log_probs.exp()\n",
    "\t# to simplify API we inline log_softmax gradient, i.e. next two lines are equivalent to: grad_logits, = torch.autograd.grad(loss, logits, retain_graph = True). gradient formula explained at https://stackoverflow.com/questions/35304393/trying-to-understand-code-that-computes-the-gradient-wrt-to-the-input-for-logsof\n",
    "\tgrad_log_probs, = torch.autograd.grad(loss, log_probs, retain_graph = retain_graph)\n",
    "\tgrad_logits = grad_log_probs - probs * grad_log_probs.sum(dim = -1, keepdim = True)\n",
    "\ttemporal_mask = (torch.arange(len(log_probs), device = input_lengths.device, dtype = input_lengths.dtype).unsqueeze(1) < input_lengths.unsqueeze(0)).unsqueeze(-1)\n",
    "\treturn (probs * temporal_mask - grad_logits).detach()\n",
    "\n",
    "def logadd(x0, x1, x2):\n",
    "\t# produces nan gradients in backward if -inf log-space zero element is used https://github.com/pytorch/pytorch/issues/31829\n",
    "\treturn torch.logsumexp(torch.stack([x0, x1, x2]), dim = 0)\n",
    "\t\n",
    "\t# use if -inf log-space zero element is used\n",
    "\t#return LogsumexpFunction.apply(x0, x1, x2)\n",
    "\t\n",
    "\t# produces inplace modification error https://github.com/pytorch/pytorch/issues/31819\n",
    "\t#m = torch.max(torch.max(x0, x1), x2)\n",
    "\t#m = m.masked_fill(torch.isinf(m), 0)\n",
    "\t#res = (x0 - m).exp() + (x1 - m).exp() + (x2 - m).exp()\n",
    "\t#return res.log().add(m)\n",
    "\n",
    "class LogsumexpFunction(torch.autograd.function.Function):\n",
    "\t@staticmethod\n",
    "\tdef forward(self, x0, x1, x2):\n",
    "\t\tm = torch.max(torch.max(x0, x1), x2)\n",
    "\t\tm = m.masked_fill_(torch.isinf(m), 0)\n",
    "\t\te0 = (x0 - m).exp_()\n",
    "\t\te1 = (x1 - m).exp_()\n",
    "\t\te2 = (x2 - m).exp_()\n",
    "\t\te = (e0 + e1 + e2).clamp_(min = 1e-16)\n",
    "\t\tself.save_for_backward(e0, e1, e2, e)\n",
    "\t\treturn e.log().add_(m)\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef backward(self, grad_output):\n",
    "\t\te0, e1, e2, e = self.saved_tensors\n",
    "\t\tg = grad_output / e\n",
    "\t\treturn g * e0, g * e1, g * e2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_probs tensor([[[-2.94, -1.06, -1.49, -2.87, -3.51, -1.90, -1.99]],\n",
      "\n",
      "        [[-3.59, -1.03, -1.59, -2.52, -1.33, -4.84, -2.86]],\n",
      "\n",
      "        [[-1.79, -0.96, -1.96, -2.73, -2.17, -2.74, -2.77]],\n",
      "\n",
      "        [[-2.31, -1.09, -4.42, -1.47, -1.62, -3.15, -2.52]],\n",
      "\n",
      "        [[-0.86, -1.69, -6.59, -2.31, -2.36, -1.67, -4.73]],\n",
      "\n",
      "        [[-2.25, -1.32, -2.11, -1.07, -2.07, -3.58, -4.68]]], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<LogSoftmaxBackward>)\n",
      "Device: cuda\n",
      "Log-probs shape (time X batch X channels): 6x1x7\n",
      "Built-in CTC loss fwd 0.0008490085601806641 bwd 0.0008418560028076172\n",
      "B, tensor([0], device='cuda:0')\n",
      "targets, tensor([[1, 2, 3]], device='cuda:0')\n",
      "targets_, tensor([[1, 2, 3, 1]], device='cuda:0')\n",
      "targets_, tensor([[0, 1, 0, 2, 0, 3, 0, 1]], device='cuda:0')\n",
      "targets_[:, 2:] != targets_[:, :-2], tensor([[False,  True, False,  True, False,  True]], device='cuda:0')\n",
      "targets_[:, 2:], tensor([[0, 2, 0, 3, 0, 1]], device='cuda:0')\n",
      "targets_[:, :-2], tensor([[0, 1, 0, 2, 0, 3]], device='cuda:0')\n",
      "diff_labels, tensor([[False, False, False,  True, False,  True, False,  True]],\n",
      "       device='cuda:0')\n",
      "zero, -1.7976931348623157e+308\n",
      "log_probs, tensor([[[-2.94, -1.06, -1.49, -2.87, -3.51, -1.90, -1.99]],\n",
      "\n",
      "        [[-3.59, -1.03, -1.59, -2.52, -1.33, -4.84, -2.86]],\n",
      "\n",
      "        [[-1.79, -0.96, -1.96, -2.73, -2.17, -2.74, -2.77]],\n",
      "\n",
      "        [[-2.31, -1.09, -4.42, -1.47, -1.62, -3.15, -2.52]],\n",
      "\n",
      "        [[-0.86, -1.69, -6.59, -2.31, -2.36, -1.67, -4.73]],\n",
      "\n",
      "        [[-2.25, -1.32, -2.11, -1.07, -2.07, -3.58, -4.68]]], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<LogSoftmaxBackward>)\n",
      "targets_.expand(len(log_probs), -1, -1), tensor([[[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]]], device='cuda:0')\n",
      "log_probs_, tensor([[[-2.94, -1.06, -2.94, -1.49, -2.94, -2.87, -2.94, -1.06]],\n",
      "\n",
      "        [[-3.59, -1.03, -3.59, -1.59, -3.59, -2.52, -3.59, -1.03]],\n",
      "\n",
      "        [[-1.79, -0.96, -1.79, -1.96, -1.79, -2.73, -1.79, -0.96]],\n",
      "\n",
      "        [[-2.31, -1.09, -2.31, -4.42, -2.31, -1.47, -2.31, -1.09]],\n",
      "\n",
      "        [[-0.86, -1.69, -0.86, -6.59, -0.86, -2.31, -0.86, -1.69]],\n",
      "\n",
      "        [[-2.25, -1.32, -2.25, -2.11, -2.25, -1.07, -2.25, -1.32]]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<GatherBackward>)\n",
      "log_probs[0, :, blank], tensor([-2.94], device='cuda:0', dtype=torch.float64, grad_fn=<SelectBackward>)\n",
      "log_alpha.size(), torch.Size([6, 1, 10])\n",
      "log_alpha, tensor([[[-1.80e+308, -1.80e+308,  -2.94e+00,  -1.06e+00, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308]]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<CopySlices>)\n",
      "log_alpha, tensor([[[-1.80e+308, -1.80e+308,  -2.94e+00,  -1.06e+00, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308,  -6.53e+00,  -1.94e+00,  -4.65e+00,\n",
      "           -2.65e+00, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308,  -8.32e+00,  -2.89e+00,  -3.66e+00,\n",
      "           -3.46e+00,  -4.43e+00,  -5.38e+00, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308,  -1.06e+01,  -3.97e+00,  -4.82e+00,\n",
      "           -6.60e+00,  -5.45e+00,  -4.51e+00,  -7.69e+00,  -6.46e+00]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308,  -1.15e+01,  -5.66e+00,  -4.47e+00,\n",
      "           -1.02e+01,  -6.03e+00,  -6.41e+00,  -5.33e+00,  -6.03e+00]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308,  -1.37e+01,  -6.98e+00,  -6.46e+00,\n",
      "           -6.31e+00,  -8.27e+00,  -6.57e+00,  -7.29e+00,  -6.04e+00]]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<CopySlices>)\n",
      "Custom CTC loss fwd 0.02208542823791504 bwd 0.0075643062591552734\n",
      "Custom loss matches: True\n",
      "Grad matches: True\n",
      "CE grad matches: True\n",
      "B, tensor([0], device='cuda:0')\n",
      "targets, tensor([[1, 2, 3]], device='cuda:0')\n",
      "targets_, tensor([[1, 2, 3, 1]], device='cuda:0')\n",
      "targets_, tensor([[0, 1, 0, 2, 0, 3, 0, 1]], device='cuda:0')\n",
      "targets_[:, 2:] != targets_[:, :-2], tensor([[False,  True, False,  True, False,  True]], device='cuda:0')\n",
      "targets_[:, 2:], tensor([[0, 2, 0, 3, 0, 1]], device='cuda:0')\n",
      "targets_[:, :-2], tensor([[0, 1, 0, 2, 0, 3]], device='cuda:0')\n",
      "diff_labels, tensor([[False, False, False,  True, False,  True, False,  True]],\n",
      "       device='cuda:0')\n",
      "zero, -1.7976931348623157e+308\n",
      "log_probs, tensor([[[-2.94, -1.06, -1.49, -2.87, -3.51, -1.90, -1.99]],\n",
      "\n",
      "        [[-3.59, -1.03, -1.59, -2.52, -1.33, -4.84, -2.86]],\n",
      "\n",
      "        [[-1.79, -0.96, -1.96, -2.73, -2.17, -2.74, -2.77]],\n",
      "\n",
      "        [[-2.31, -1.09, -4.42, -1.47, -1.62, -3.15, -2.52]],\n",
      "\n",
      "        [[-0.86, -1.69, -6.59, -2.31, -2.36, -1.67, -4.73]],\n",
      "\n",
      "        [[-2.25, -1.32, -2.11, -1.07, -2.07, -3.58, -4.68]]], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<LogSoftmaxBackward>)\n",
      "targets_.expand(len(log_probs), -1, -1), tensor([[[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 2, 0, 3, 0, 1]]], device='cuda:0')\n",
      "log_probs_, tensor([[[-2.94, -1.06, -2.94, -1.49, -2.94, -2.87, -2.94, -1.06]],\n",
      "\n",
      "        [[-3.59, -1.03, -3.59, -1.59, -3.59, -2.52, -3.59, -1.03]],\n",
      "\n",
      "        [[-1.79, -0.96, -1.79, -1.96, -1.79, -2.73, -1.79, -0.96]],\n",
      "\n",
      "        [[-2.31, -1.09, -2.31, -4.42, -2.31, -1.47, -2.31, -1.09]],\n",
      "\n",
      "        [[-0.86, -1.69, -0.86, -6.59, -0.86, -2.31, -0.86, -1.69]],\n",
      "\n",
      "        [[-2.25, -1.32, -2.25, -2.11, -2.25, -1.07, -2.25, -1.32]]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<GatherBackward>)\n",
      "log_probs[0, :, blank], tensor([-2.94], device='cuda:0', dtype=torch.float64, grad_fn=<SelectBackward>)\n",
      "log_alpha.size(), torch.Size([6, 1, 10])\n",
      "log_alpha, tensor([[[-1.80e+308, -1.80e+308,  -2.94e+00,  -1.06e+00, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308]]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<CopySlices>)\n",
      "log_alpha, tensor([[[-1.80e+308, -1.80e+308,  -2.94e+00,  -1.06e+00, -1.80e+308,\n",
      "          -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308,  -6.53e+00,  -1.94e+00,  -4.65e+00,\n",
      "           -2.65e+00, -1.80e+308, -1.80e+308, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308,  -8.32e+00,  -2.89e+00,  -3.66e+00,\n",
      "           -3.46e+00,  -4.43e+00,  -5.38e+00, -1.80e+308, -1.80e+308]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308,  -1.06e+01,  -3.97e+00,  -4.82e+00,\n",
      "           -6.60e+00,  -5.45e+00,  -4.51e+00,  -7.69e+00,  -6.46e+00]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308,  -1.15e+01,  -5.66e+00,  -4.47e+00,\n",
      "           -1.02e+01,  -6.03e+00,  -6.41e+00,  -5.33e+00,  -6.03e+00]],\n",
      "\n",
      "        [[-1.80e+308, -1.80e+308,  -1.37e+01,  -6.98e+00,  -6.46e+00,\n",
      "           -6.31e+00,  -8.27e+00,  -6.57e+00,  -7.29e+00,  -6.04e+00]]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<CopySlices>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test abc-3\n",
    "\n",
    "probs_seq1 = [[\n",
    "            0.06390443, 0.42124858, 0.27323887, 0.06870235, 0.0361254,\n",
    "            0.18184413, 0.16493624\n",
    "        ], [\n",
    "            0.03309247, 0.42866108, 0.24390638, 0.09699597, 0.31895462,\n",
    "            0.0094893, 0.06890021\n",
    "        ], [\n",
    "            0.218104, 0.49992557, 0.18245131, 0.08503348, 0.14903535,\n",
    "            0.08424043, 0.08120984\n",
    "        ], [\n",
    "            0.12094152, 0.41162472, 0.01473646, 0.28045061, 0.24246305,\n",
    "            0.05206269, 0.09772094\n",
    "        ], [\n",
    "            0.9333387, 0.40550838, 0.00301669, 0.21745861, 0.20803985,\n",
    "            0.41317442, 0.01946335\n",
    "        ], [\n",
    "            0.16468227, 0.4180699, 0.1906545, 0.53963251, 0.19860937,\n",
    "            0.04377724, 0.01457421\n",
    "        ]]\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "torch.set_printoptions(precision=2)\n",
    "\n",
    "\n",
    "# import ctc\n",
    "\n",
    "T, B, C = 6, 1, 7\n",
    "t = 3\n",
    "blank = 0\n",
    "device = 'cuda'\n",
    "seed = 1\n",
    "atol = 1e-3\n",
    "for set_seed in [torch.manual_seed] + ([torch.cuda.manual_seed_all] if device == 'cuda' else []):\n",
    "\tset_seed(seed)\n",
    "tictoc = lambda: (device == 'cuda' and torch.cuda.synchronize()) or time.time()\n",
    "\n",
    "# logits = torch.randn(T, B, C, device = device).requires_grad_()\n",
    "import numpy\n",
    "logits = torch.reshape(torch.as_tensor(numpy.array(numpy.log(probs_seq1)),device = device), (6,1,7)).requires_grad_()\n",
    "\n",
    "# targets = torch.randint(blank + 1, C, (B, t), dtype = torch.long, device = device)\n",
    "targets = torch.reshape(torch.as_tensor(numpy.array([1,2,3]), dtype = torch.long, device = device), (B, t))\n",
    "\n",
    "\n",
    "\n",
    "input_lengths = torch.full((B,), T, dtype = torch.long, device = device)\n",
    "target_lengths = torch.full((B,), t, dtype = torch.long, device = device)\n",
    "log_probs = logits.log_softmax(dim = -1)\n",
    "print(\"log_probs\",log_probs)\n",
    "print('Device:', device)\n",
    "print('Log-probs shape (time X batch X channels):', 'x'.join(map(str, log_probs.shape)))\n",
    "\n",
    "tic = tictoc()\n",
    "builtin_ctc = F.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank = 0, reduction = 'none')\n",
    "toc = tictoc()\n",
    "builtin_ctc_grad, = torch.autograd.grad(builtin_ctc.sum(), logits, retain_graph = True)\n",
    "print('Built-in CTC loss', 'fwd', toc - tic, 'bwd', tictoc() - toc)\n",
    "\n",
    "tic = tictoc()\n",
    "custom_ctc = ctc_loss(log_probs, targets, input_lengths, target_lengths, blank = 0, reduction = 'none')\n",
    "toc = tictoc()\n",
    "custom_ctc_grad, = torch.autograd.grad(custom_ctc.sum(), logits, retain_graph = True)\n",
    "print('Custom CTC loss', 'fwd', toc - tic, 'bwd', tictoc() - toc)\n",
    "\n",
    "ce_alignment_targets = ctc_alignment_targets(log_probs, targets, input_lengths, target_lengths, blank = 0)\n",
    "ce_ctc = -ce_alignment_targets * log_probs\n",
    "ce_ctc_grad, = torch.autograd.grad(ce_ctc.sum(), logits, retain_graph = True)\n",
    "\n",
    "print('Custom loss matches:', torch.allclose(builtin_ctc, custom_ctc, atol = atol))\n",
    "print('Grad matches:', torch.allclose(builtin_ctc_grad, custom_ctc_grad, atol = atol))\n",
    "print('CE grad matches:', torch.allclose(builtin_ctc_grad, ce_ctc_grad, atol = atol))\n",
    "\n",
    "alignment = ctc_loss(log_probs, targets, input_lengths, target_lengths, blank = 0, reduction = 'none', alignment = True)\n",
    "a = alignment[:, 0, :target_lengths[0]]\n",
    "# print(a.t().cpu())\n",
    "plt.subplot(211)\n",
    "plt.title('Input-Output Viterbi alignment')\n",
    "plt.imshow(a.t().cpu(), origin = 'lower', aspect = 'auto')\n",
    "plt.xlabel('Input steps')\n",
    "plt.ylabel('Output steps')\n",
    "plt.subplot(212)\n",
    "plt.title('CTC alignment targets')\n",
    "a = ce_alignment_targets[:, 0, :]\n",
    "plt.imshow(a.t().cpu(), origin = 'lower', aspect = 'auto')\n",
    "plt.xlabel('Input steps')\n",
    "plt.ylabel(f'Output symbols, blank {blank}')\n",
    "plt.subplots_adjust(hspace = 0.5)\n",
    "plt.savefig('alignment.png')\n",
    "torch.set_printoptions(precision=2)\n",
    "# print(a.t().cpu())\n",
    "# print(targets[:,0])\n",
    "# print(logits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3]], device='cuda:0') tensor([[1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(targets, targets[:, :1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([[1,2],[3,4]])\n",
    "r = torch.gather(t, 1, torch.tensor([[0,0],[1,0]]))\n",
    "index = [[0,0],[1,0]]\n",
    "i,j,k = 0,0,1\n",
    "print(t[index[j][k]][k] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1],\n",
      "        [4, 3]])\n"
     ]
    }
   ],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1],\n",
       "        [4, 3]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.gather(1, torch.tensor([[0,0],[1,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1],\n",
       "        [4, 3]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.gather(-1, torch.tensor([[0,0],[1,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
