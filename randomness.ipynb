{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_seq1 = [[\n",
    "            0.06390443, 0.42124858, 0.27323887, 0.06870235, 0.0361254,\n",
    "            0.18184413, 0.16493624\n",
    "        ], [\n",
    "            0.03309247, 0.42866108, 0.24390638, 0.09699597, 0.31895462,\n",
    "            0.0094893, 0.06890021\n",
    "        ], [\n",
    "            0.218104, 0.49992557, 0.18245131, 0.08503348, 0.14903535,\n",
    "            0.08424043, 0.08120984\n",
    "        ], [\n",
    "            0.12094152, 0.41162472, 0.01473646, 0.28045061, 0.24246305,\n",
    "            0.05206269, 0.09772094\n",
    "        ], [\n",
    "            0.9333387, 0.40550838, 0.00301669, 0.21745861, 0.20803985,\n",
    "            0.41317442, 0.01946335\n",
    "        ], [\n",
    "            0.16468227, 0.4180699, 0.1906545, 0.53963251, 0.19860937,\n",
    "            0.04377724, 0.01457421\n",
    "        ]]\n",
    "\n",
    "probs_seq2 = [[\n",
    "            1.06390443, 0.42124858, 0.27323887, 0.06870235, 0.0361254,\n",
    "            0.18184413, 0.16493624\n",
    "        ], [\n",
    "            0.03309247, 0.42866108, 0.24390638, 0.09699597, 0.31895462,\n",
    "            0.0094893, 0.06890021\n",
    "        ], [\n",
    "            0.218104, 0.49992557, 0.18245131, 0.08503348, 0.14903535,\n",
    "            0.08424043, 0.08120984\n",
    "        ], [\n",
    "            0.12094152, 0.41162472, 0.01473646, 0.28045061, 0.24246305,\n",
    "            0.05206269, 0.09772094\n",
    "        ], [\n",
    "            0.9333387, 0.40550838, 0.00301669, 0.21745861, 0.20803985,\n",
    "            0.41317442, 0.01946335\n",
    "        ], [\n",
    "            0.16468227, 0.4180699, 0.1906545, 0.53963251, 0.19860937,\n",
    "            0.04377724, 0.01457421\n",
    "        ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_path(prob, path):\n",
    "    import ctc\n",
    "    import time\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    torch.set_printoptions(precision=2)\n",
    "\n",
    "\n",
    "    # import ctc\n",
    "\n",
    "    T, B, C = len(prob), 1, len(prob[0])\n",
    "    t = len(path)\n",
    "    blank = 0\n",
    "    device = 'cuda'\n",
    "    seed = 1\n",
    "    atol = 1e-3\n",
    "#     for set_seed in [torch.manual_seed] + ([torch.cuda.manual_seed_all] if device == 'cuda' else []):\n",
    "#         set_seed(seed)\n",
    "    tictoc = lambda: (device == 'cuda' and torch.cuda.synchronize()) or time.time()\n",
    "\n",
    "    # logits = torch.randn(T, B, C, device = device).requires_grad_()\n",
    "    import numpy\n",
    "    logits = torch.reshape(torch.as_tensor(numpy.array(numpy.log(prob)),device = device), (T,B,C)).requires_grad_()\n",
    "\n",
    "    # targets = torch.randint(blank + 1, C, (B, t), dtype = torch.long, device = device)\n",
    "    targets = torch.reshape(torch.as_tensor(path, dtype = torch.long, device = device), (B, t))\n",
    "\n",
    "\n",
    "\n",
    "    input_lengths = torch.full((B,), T, dtype = torch.long, device = device)\n",
    "    target_lengths = torch.full((B,), t, dtype = torch.long, device = device)\n",
    "    log_probs = logits.log_softmax(dim = -1)\n",
    "    print(log_probs)\n",
    "    if print_flag:\n",
    "        print(\"log_probs\",log_probs)\n",
    "    if print_flag:\n",
    "        print('Device:', device)\n",
    "    if print_flag:\n",
    "        print('Log-probs shape (time X batch X channels):', 'x'.join(map(str, log_probs.shape)))\n",
    "\n",
    "    tic = tictoc()\n",
    "    builtin_ctc = F.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank = 0, reduction = 'none')\n",
    "    print(builtin_ctc)\n",
    "    toc = tictoc()\n",
    "    builtin_ctc_grad, = torch.autograd.grad(builtin_ctc.sum(), logits, retain_graph = True)\n",
    "    print('Built-in CTC loss', 'fwd', toc - tic, 'bwd', tictoc() - toc)\n",
    "\n",
    "#     tic = tictoc()\n",
    "#     custom_ctc = ctc_loss(log_probs, targets, input_lengths, target_lengths, blank = 0, reduction = 'none')\n",
    "#     toc = tictoc()\n",
    "#     custom_ctc_grad, = torch.autograd.grad(custom_ctc.sum(), logits, retain_graph = True)\n",
    "#     print('Custom CTC loss', 'fwd', toc - tic, 'bwd', tictoc() - toc)\n",
    "\n",
    "#     ce_alignment_targets = ctc_alignment_targets(log_probs, targets, input_lengths, target_lengths, blank = 0)\n",
    "#     ce_ctc = -ce_alignment_targets * log_probs\n",
    "#     ce_ctc_grad, = torch.autograd.grad(ce_ctc.sum(), logits, retain_graph = True)\n",
    "\n",
    "#     print('Custom loss matches:', torch.allclose(builtin_ctc, custom_ctc, atol = atol))\n",
    "#     print('Grad matches:', torch.allclose(builtin_ctc_grad, custom_ctc_grad, atol = atol))\n",
    "#     print('CE grad matches:', torch.allclose(builtin_ctc_grad, ce_ctc_grad, atol = atol))\n",
    "\n",
    "#     alignment = ctc_loss(log_probs, targets, input_lengths, target_lengths, blank = 0, reduction = 'none', alignment = True)\n",
    "#     a = alignment[:, 0, :target_lengths[0]]\n",
    "#     # print(a.t().cpu())\n",
    "#     plt.subplot(311)\n",
    "#     plt.title('Input-Output maxt CTC Viterbi alignment')\n",
    "#     plt.imshow(a.t().cpu(), origin = 'lower', aspect = 'auto')\n",
    "#     plt.xlabel('Input steps')\n",
    "#     plt.ylabel('Output steps')\n",
    "#     plt.subplot(312)\n",
    "#     plt.title('CTC alignment targets')\n",
    "#     a = ce_alignment_targets[:, 0, :]\n",
    "#     plt.imshow(a.t().cpu(), origin = 'lower', aspect = 'auto')\n",
    "#     plt.xlabel('Input steps')\n",
    "#     plt.ylabel(f'Output symbols, blank {blank}')\n",
    "#     plt.subplots_adjust(hspace = 0.5)\n",
    "#     plt.savefig('alignment.png')\n",
    "#     torch.set_printoptions(precision=2)\n",
    "#     # print(a.t().cpu())\n",
    "#     # print(targets[:,0])\n",
    "#     # print(logits)\n",
    "\n",
    "#     plt.subplot(313)\n",
    "#     alignment = ctc_max_loss(log_probs, targets, input_lengths, target_lengths, blank = 0, reduction = 'none', alignment = True)\n",
    "#     a = alignment[:, 0, :target_lengths[0]]\n",
    "#     # print(a.t().cpu())\n",
    "#     plt.title('Input-Output max prob. Viterbi alignment')\n",
    "#     plt.imshow(a.t().cpu(), origin = 'lower', aspect = 'auto')\n",
    "#     plt.xlabel('Input steps')\n",
    "#     plt.ylabel('Output steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.73, -1.66, -2.09, -3.47, -4.11, -2.50, -2.60]],\n",
      "\n",
      "        [[-3.59, -1.03, -1.59, -2.52, -1.33, -4.84, -2.86]],\n",
      "\n",
      "        [[-1.79, -0.96, -1.96, -2.73, -2.17, -2.74, -2.77]],\n",
      "\n",
      "        [[-2.31, -1.09, -4.42, -1.47, -1.62, -3.15, -2.52]],\n",
      "\n",
      "        [[-0.86, -1.69, -6.59, -2.31, -2.36, -1.67, -4.73]],\n",
      "\n",
      "        [[-2.25, -1.32, -2.11, -1.07, -2.07, -3.58, -4.68]]], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([6.07], device='cuda:0', dtype=torch.float64, grad_fn=<CtcLossBackward>)\n",
      "Built-in CTC loss fwd 0.0010409355163574219 bwd 0.00039005279541015625\n"
     ]
    }
   ],
   "source": [
    "print_flag = False\n",
    "find_path(probs_seq2, [1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 6, 3]], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.manual_seed(66)\n",
    "torch.randint(blank + 1, C, (B, t), dtype = torch.long, device = device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
