{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Hotel of California\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_sec = 5*60+41\n",
    "lenght_char = 1742"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_string(text):\n",
    "    from collections import defaultdict\n",
    "    # text = 'Mary had a little lamb'.replace(\" \",\"\")\n",
    "    chars = defaultdict(int)\n",
    "\n",
    "    for char in text:\n",
    "        chars[char.lower()] += 1\n",
    "    return sum(chars.values())\n",
    "#     print(chars)\n",
    "#     print(sum(chars.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1742\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "with open('/root/Montreal-Forced-Aligner/Hotel/Hotel_California.lab') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        cnt += sum_string(line)\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 31.72 GiB total capacity; 29.09 GiB already allocated; 3.94 MiB free; 64.05 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-db18989ea73c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtictoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m \u001b[0malignment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'none'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malignment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Alignment: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtictoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ctc/ctc.py\u001b[0m in \u001b[0;36mctc_loss\u001b[0;34m(log_probs, targets, input_lengths, target_lengths, blank, reduction, alignment)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# log_alpha[1:, :, zero_padding:] = log_probs.gather(-1, targets_.expand(len(log_probs), -1, -1))[1:]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                 \u001b[0mlog_alpha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_probs_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlogadd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_alpha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_alpha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_alpha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0ml1l2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_alpha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_lengths\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mzero_padding\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtarget_lengths\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_padding\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtarget_lengths\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ctc/ctc.py\u001b[0m in \u001b[0;36mlogadd\u001b[0;34m(x0, x1, x2)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlogadd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# produces nan gradients in backward if -inf log-space zero element is used https://github.com/pytorch/pytorch/issues/31829\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogsumexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# use if -inf log-space zero element is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 31.72 GiB total capacity; 29.09 GiB already allocated; 3.94 MiB free; 64.05 MiB cached)"
     ]
    }
   ],
   "source": [
    "# test abc-3\n",
    "\n",
    "# probs_seq1 = [[\n",
    "#             0.06390443, 0.42124858, 0.27323887, 0.06870235, 0.0361254,\n",
    "#             0.18184413, 0.16493624\n",
    "#         ], [\n",
    "#             0.03309247, 0.42866108, 0.24390638, 0.09699597, 0.31895462,\n",
    "#             0.0094893, 0.06890021\n",
    "#         ], [\n",
    "#             0.218104, 0.49992557, 0.18245131, 0.08503348, 0.14903535,\n",
    "#             0.08424043, 0.08120984\n",
    "#         ], [\n",
    "#             0.12094152, 0.41162472, 0.01473646, 0.28045061, 0.24246305,\n",
    "#             0.05206269, 0.09772094\n",
    "#         ], [\n",
    "#             0.9333387, 0.40550838, 0.00301669, 0.21745861, 0.20803985,\n",
    "#             0.41317442, 0.01946335\n",
    "#         ], [\n",
    "#             0.16468227, 0.4180699, 0.1906545, 0.53963251, 0.19860937,\n",
    "#             0.04377724, 0.01457421\n",
    "#         ]]\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "torch.set_printoptions(precision=2)\n",
    "\n",
    "\n",
    "import ctc\n",
    "\n",
    "T, B, C = length_sec*2205, 1, 27\n",
    "t = lenght_char\n",
    "blank = 0\n",
    "device = 'cuda'\n",
    "seed = 1\n",
    "atol = 1e-3\n",
    "for set_seed in [torch.manual_seed] + ([torch.cuda.manual_seed_all] if device == 'cuda' else []):\n",
    "\tset_seed(seed)\n",
    "tictoc = lambda: (device == 'cuda' and torch.cuda.synchronize()) or time.time()\n",
    "\n",
    "logits = torch.randn(T, B, C, device = device).requires_grad_()\n",
    "# import numpy\n",
    "# logits = torch.reshape(torch.as_tensor(numpy.array(numpy.log(probs_seq1)),device = device), (6,1,7)).requires_grad_()\n",
    "\n",
    "targets = torch.randint(blank + 1, C, (B, t), dtype = torch.long, device = device)\n",
    "# targets = torch.reshape(torch.as_tensor(numpy.array([1,2,3]), dtype = torch.long, device = device), (B, t))\n",
    "\n",
    "\n",
    "\n",
    "input_lengths = torch.full((B,), T, dtype = torch.long, device = device)\n",
    "target_lengths = torch.full((B,), t, dtype = torch.long, device = device)\n",
    "log_probs = logits.log_softmax(dim = -1)\n",
    "# print(\"log_probs\",log_probs)\n",
    "# print('Device:', device)\n",
    "# print('Log-probs shape (time X batch X channels):', 'x'.join(map(str, log_probs.shape)))\n",
    "\n",
    "# tic = tictoc()\n",
    "# builtin_ctc = F.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank = 0, reduction = 'none')\n",
    "# toc = tictoc()\n",
    "# builtin_ctc_grad, = torch.autograd.grad(builtin_ctc.sum(), logits, retain_graph = True)\n",
    "# print('Built-in CTC loss', 'fwd', toc - tic, 'bwd', tictoc() - toc)\n",
    "\n",
    "# tic = tictoc()\n",
    "# custom_ctc = ctc_loss(log_probs, targets, input_lengths, target_lengths, blank = 0, reduction = 'none')\n",
    "# toc = tictoc()\n",
    "# custom_ctc_grad, = torch.autograd.grad(custom_ctc.sum(), logits, retain_graph = True)\n",
    "# print('Custom CTC loss', 'fwd', toc - tic, 'bwd', tictoc() - toc)\n",
    "\n",
    "# ce_alignment_targets = ctc_alignment_targets(log_probs, targets, input_lengths, target_lengths, blank = 0)\n",
    "# ce_ctc = -ce_alignment_targets * log_probs\n",
    "# ce_ctc_grad, = torch.autograd.grad(ce_ctc.sum(), logits, retain_graph = True)\n",
    "\n",
    "# print('Custom loss matches:', torch.allclose(builtin_ctc, custom_ctc, atol = atol))\n",
    "# print('Grad matches:', torch.allclose(builtin_ctc_grad, custom_ctc_grad, atol = atol))\n",
    "# print('CE grad matches:', torch.allclose(builtin_ctc_grad, ce_ctc_grad, atol = atol))\n",
    "\n",
    "tic = tictoc()\n",
    "alignment = ctc.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank = 0, reduction = 'none', alignment = True)\n",
    "print('Alignment: ',tictoc() - tic)\n",
    "\n",
    "a = alignment[:, 0, :target_lengths[0]]\n",
    "# print(a.t().cpu())\n",
    "plt.subplot(211)\n",
    "plt.title('Input-Output Viterbi alignment')\n",
    "plt.imshow(a.t().cpu(), origin = 'lower', aspect = 'auto')\n",
    "plt.xlabel('Input steps')\n",
    "plt.ylabel('Output steps')\n",
    "# plt.subplot(212)\n",
    "# plt.title('CTC alignment targets')\n",
    "# a = ce_alignment_targets[:, 0, :]\n",
    "# plt.imshow(a.t().cpu(), origin = 'lower', aspect = 'auto')\n",
    "# plt.xlabel('Input steps')\n",
    "# plt.ylabel(f'Output symbols, blank {blank}')\n",
    "# plt.subplots_adjust(hspace = 0.5)\n",
    "# plt.savefig('alignment.png')\n",
    "# torch.set_printoptions(precision=2)\n",
    "# print(a.t().cpu())\n",
    "# print(targets[:,0])\n",
    "# print(logits)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test abc-3\n",
    "\n",
    "# probs_seq1 = [[\n",
    "#             0.06390443, 0.42124858, 0.27323887, 0.06870235, 0.0361254,\n",
    "#             0.18184413, 0.16493624\n",
    "#         ], [\n",
    "#             0.03309247, 0.42866108, 0.24390638, 0.09699597, 0.31895462,\n",
    "#             0.0094893, 0.06890021\n",
    "#         ], [\n",
    "#             0.218104, 0.49992557, 0.18245131, 0.08503348, 0.14903535,\n",
    "#             0.08424043, 0.08120984\n",
    "#         ], [\n",
    "#             0.12094152, 0.41162472, 0.01473646, 0.28045061, 0.24246305,\n",
    "#             0.05206269, 0.09772094\n",
    "#         ], [\n",
    "#             0.9333387, 0.40550838, 0.00301669, 0.21745861, 0.20803985,\n",
    "#             0.41317442, 0.01946335\n",
    "#         ], [\n",
    "#             0.16468227, 0.4180699, 0.1906545, 0.53963251, 0.19860937,\n",
    "#             0.04377724, 0.01457421\n",
    "#         ]]\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "torch.set_printoptions(precision=2)\n",
    "\n",
    "\n",
    "import ctc\n",
    "\n",
    "T, B, C = length_sec*2205, 1, 27\n",
    "t = lenght_char\n",
    "blank = 0\n",
    "device = 'cpu'\n",
    "seed = 1\n",
    "atol = 1e-3\n",
    "for set_seed in [torch.manual_seed] + ([torch.cuda.manual_seed_all] if device == 'cuda' else []):\n",
    "\tset_seed(seed)\n",
    "tictoc = lambda: (device == 'cuda' and torch.cuda.synchronize()) or time.time()\n",
    "\n",
    "logits = torch.randn(T, B, C, device = device).requires_grad_()\n",
    "# import numpy\n",
    "# logits = torch.reshape(torch.as_tensor(numpy.array(numpy.log(probs_seq1)),device = device), (6,1,7)).requires_grad_()\n",
    "\n",
    "targets = torch.randint(blank + 1, C, (B, t), dtype = torch.long, device = device)\n",
    "# targets = torch.reshape(torch.as_tensor(numpy.array([1,2,3]), dtype = torch.long, device = device), (B, t))\n",
    "\n",
    "\n",
    "\n",
    "input_lengths = torch.full((B,), T, dtype = torch.long, device = device)\n",
    "target_lengths = torch.full((B,), t, dtype = torch.long, device = device)\n",
    "log_probs = logits.log_softmax(dim = -1)\n",
    "# print(\"log_probs\",log_probs)\n",
    "# print('Device:', device)\n",
    "# print('Log-probs shape (time X batch X channels):', 'x'.join(map(str, log_probs.shape)))\n",
    "\n",
    "# tic = tictoc()\n",
    "# builtin_ctc = F.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank = 0, reduction = 'none')\n",
    "# toc = tictoc()\n",
    "# builtin_ctc_grad, = torch.autograd.grad(builtin_ctc.sum(), logits, retain_graph = True)\n",
    "# print('Built-in CTC loss', 'fwd', toc - tic, 'bwd', tictoc() - toc)\n",
    "\n",
    "# tic = tictoc()\n",
    "# custom_ctc = ctc_loss(log_probs, targets, input_lengths, target_lengths, blank = 0, reduction = 'none')\n",
    "# toc = tictoc()\n",
    "# custom_ctc_grad, = torch.autograd.grad(custom_ctc.sum(), logits, retain_graph = True)\n",
    "# print('Custom CTC loss', 'fwd', toc - tic, 'bwd', tictoc() - toc)\n",
    "\n",
    "# ce_alignment_targets = ctc_alignment_targets(log_probs, targets, input_lengths, target_lengths, blank = 0)\n",
    "# ce_ctc = -ce_alignment_targets * log_probs\n",
    "# ce_ctc_grad, = torch.autograd.grad(ce_ctc.sum(), logits, retain_graph = True)\n",
    "\n",
    "# print('Custom loss matches:', torch.allclose(builtin_ctc, custom_ctc, atol = atol))\n",
    "# print('Grad matches:', torch.allclose(builtin_ctc_grad, custom_ctc_grad, atol = atol))\n",
    "# print('CE grad matches:', torch.allclose(builtin_ctc_grad, ce_ctc_grad, atol = atol))\n",
    "\n",
    "tic = tictoc()\n",
    "alignment = ctc.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank = 0, reduction = 'none', alignment = True)\n",
    "print('Alignment: ',tictoc() - tic)\n",
    "\n",
    "a = alignment[:, 0, :target_lengths[0]]\n",
    "# print(a.t().cpu())\n",
    "plt.subplot(211)\n",
    "plt.title('Input-Output Viterbi alignment')\n",
    "plt.imshow(a.t().cpu(), origin = 'lower', aspect = 'auto')\n",
    "plt.xlabel('Input steps')\n",
    "plt.ylabel('Output steps')\n",
    "# plt.subplot(212)\n",
    "# plt.title('CTC alignment targets')\n",
    "# a = ce_alignment_targets[:, 0, :]\n",
    "# plt.imshow(a.t().cpu(), origin = 'lower', aspect = 'auto')\n",
    "# plt.xlabel('Input steps')\n",
    "# plt.ylabel(f'Output symbols, blank {blank}')\n",
    "# plt.subplots_adjust(hspace = 0.5)\n",
    "# plt.savefig('alignment.png')\n",
    "# torch.set_printoptions(precision=2)\n",
    "# print(a.t().cpu())\n",
    "# print(targets[:,0])\n",
    "# print(logits)\n",
    "\n",
    "\n",
    "\"crash!!!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment:  16.461628437042236\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Output steps')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAACgCAYAAAD9/EDKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAYnElEQVR4nO3de5hcVZnv8e+PJCRIgKQNYCBIQDk+Jg4EiFwGRUYYLvGCKKMgDAiM4AhH1CNjQGfMoCOI48HBG8LIJAHkFmSIIxgDCgFFAsEAQURCuCQkJAJCAmhIwjt/rNVmp1JdtdNd1V3V/fs8Tz21a+3bu2p311t7rV1rKyIwMzOrZbO+DsDMzFqfk4WZmdXlZGFmZnU5WZiZWV1OFmZmVpeThZmZ1eVkYdYLJJ0j6T8bvM2Q9OYu5h0n6Wfd3O5USV/J0++U9EhP4rT+wcnCkPSEpEN6YT9TJF1RYrkxkq6U9JyklyXNlfTeTdjPxyTd2bNoy29P0vclTa9Svruk1ZI6IuKrEfEPuXxs/qAf3KgYK0XElRFxaAO2c0dEvKURMfWm3vqbHkicLKylSOoA7gReBcYDo4ALgR9KOrovY6thKvBBSVtWlJ8A/E9EPN/InTUzyZh1KSL8GOAP4AngkDz9MdKH9b8DfwQeB44oLHsbcB4wF3gRuBHoyPMOApZU2zZwOCkBrAFeAu7vIpYvAwuAzSrKPw88CQgYCwQwuCKufwDeCvwZWJf380KePxW4GJgNrAJuB3bO8zZ5e1XifgQ4ofB6ELAUeH9+PQW4Ik8/lff3Un7sn8tPBh7O7/uszvjyvABOBx4FHi+UfQpYBDwLfL3zfes8jjWO+XXAM/kYzgHGF+ZNBb5S7ZgCewG/ye/hdcA1lcsC/w9YASwDTqrY7neBm3O9fwm8AfhmrvPvgD0Ly+8AXA/8gfR3+KnCvCnAtcD0HMtDwMQ873LgNeBPeT//1Nf/Y/3h4TMLq2Zf0offKOAC4AeSVJh/AumDbQdgLXBRvQ1GxE+BrwLXRMTwiNiji0X/Frg+Il6rKL8WeCPwf+rs52HgE8BdeT8jCrOPIyWjUcB84MoScdfaXtF00vvS6RBgCOmDsdKB+XlE3uZdkj4AnAN8ENgWuAO4qmK9D5COzbhC2VHARNKH+JGk41LGzcBuwHbAfZR4LyRtDtxA+tDvyPEdVbHYG4BtgB2BU4DvSBpZmP9h4IukY7AauCvvfxQwA/j/eV+bAT8G7s/bOhj4tKTDCtt6P3A1MAKYCXwbICL+npSQ35ff3wvq1c3qc7Kwap6MiEsjYh0wDRgNbF+Yf3lELIiIl4F/Bj4saVCD9j2K9I200rLC/O76SUTMiYjVwBeA/SXt1IPtFV0OvEvSmPz6BOCHEbGm5PqnAedFxMMRsZaUWCdI2rmwzHkR8XxE/KlQ9rVc9hTpG/qxZXYWEZdFxKr8XkwB9pC0TZ3V9gMGAxdFxJqI+BHpDLNoDXBunn8T6Zt9sc/jhoiYFxF/JiWeP0fE9Py3dg2wZ17u7cC2EXFuRLwaEYuAS4FjCtu6MyJuyuteDnT1BcQawMnCqnmmcyIiXsmTwwvzFxemnyR9g97kD/F8pc1L+fFQLn6WlJwqjS7M766/xB0RLwHPk86Oeix/WM8Bjpc0nHQWMG0TNrEz8B+SXpD0Qo5NpG/VnRZXWa/yWNStj6RBks6X9JiklaSmQqh/DHcAno6I4uijlTE9l5Ndp1fY8G9neWH6T1Vedy67M7BD5/uR35Nz2PBLyzOF6VeAYe7PaR4nC+uO4rfxN5K+TT4LvAy8rnNGPtvYtrDsBkMcR7rSZnh+jM/FtwAfys0QRR8mfTD9Pu+H4r5IzR9V91Mt7vyB3kHqV+ju9ipNI51RfIjUr3BfF8tV295i4LSIGFF4bBERv6qzXuWxWFoizo+SmqwOITUZjc3l6mqFbBmwY0WTZKPOzCotJr2Hxfdjq4iYVHJ9D6fdYE4W1h3HSxon6XXAucCM3BTwe9K3u/dIGkJqmx5aWG85MLZKIii6ENia1E/yBknDJB1LajY6K5I/AE/nOAZJOhl4U8V+xuQ29qJJkt6Ry78M3B0Ri3uwvUrXkz48/5XaZxV/IHXA7loouxg4W9J4AEnbSPq7OvsDOEvSyNycdiapKaeerUj9Bc+REuRXS6wDqX9hHXCGpMGSjgT2KbnuppoLrJT0eUlb5OPyNklvL7n+cjZ8f62HnCysOy4ndXI+AwwjXZFDRLwIfBL4T9KH78ukq2M6XZefn5NU9Vt3RDwHvCNv97ekD7TPAn8fEcUPwo8DZ+X544HiN/Cfk66OeUZSsdnqh8CXSE08e5M6vHuyvcrYX2Z9wuiywzg37f0b8MvcxLJfRNwAfA24OjcNLQCO6GobBTcC80gd9j8BflBinemkJqunSe/xr0usQ0S8SuqAPwV4ATge+B9S4mmo/OXjfcAE0pVQz5L+rur1q3Q6D/hifn8/1+j4BiJt2PxoVpuk20iXgDb018jNJmkq6RLQL/Z1LP2JpLuBiyPiv/o6Fmsun1mYWWmS3pWbBwdLOhHYHfhpX8dlzecrB8xsU7yF9JuX4cBjwNERUe1SZ+tn3AxlZmZ1uRnKzMzqcrIwM7O6+mWfxeYaGsOoHADUzMxqWcUfn42IbavN65fJYhhbsq8O7uswzMzayi0x48mu5rkZyszM6nKyMDOzupwszMysLicLMzOry8nCzMzqcrIwM7O6nCzMzKwuJwszsz40a+n8vg6hlKYlC0mXSVohaUGhbIqkpyXNz49JhXlnS1oo6RFJhxXKD89lCyVNbla8ZmZ94bAdJvR1CKU088xiKnB4lfILI2JCftwEIGkccAzpDmWHA9/Nt1EcBHyHdMewccCxeVkzM+tFTRvuIyLmSBpbcvEjgasjYjXwuKSFrL+378KIWAQg6eq87G8bHK6ZmdXQF30WZ0h6IDdTjcxlOwKLC8ssyWVdlZuZWS/q7WTxPeBNpJuwLwO+kctVZdmoUb4RSadKulfSvWsaf/94M7MBrVeTRUQsj4h1EfEacCnrm5qWADsVFh0DLK1RXm3bl0TExIiYOIShjQ/ezGwA69VkIWl04eVRQOeVUjOBYyQNlbQLsBswF7gH2E3SLpI2J3WCz+zNmM3MrIkd3JKuAg4CRklaAnwJOEjSBFJT0hPAaQAR8ZCka0kd12uB0yNiXd7OGcAsYBBwWUQ81KyYzcysOkVU7QJoa1urI3zzIzOrZdbS+W3zG4feckvMmBcRE6vNq9sMJelMSVsr+YGk+yQd2vgwzcx6jxPFpinTZ3FyRKwEDgW2BU4Czm9qVGZm1lLKJIvOy1cnAf8VEfdT/ZJWMzPrp8oki3mSfkZKFrMkbQW81tywzMyslZS5GuoU0o/oFkXEK5JeT2qKMjOzAaJusoiI1/IYT8dLCuDOiLih2YGZmVnrKHM11HeBTwAPkn5Ed5qk7zQ7MDMzax1lmqHeBbwt8g8yJE0jJQ4zMxsgynRwPwK8sfB6J+CB5oRjZmatqMyZxeuBhyXNza/fDtwlaSZARLy/WcGZmVlrKJMs/qXpUZiZWUsrczXU7ZJ2BnaLiFskbQEMjohVzQ/PzMxaQZmroT4OzAC+n4vGAP/dzKDMzKy1lOngPh04AFgJEBGPAts1MygzM2stZZLF6oh4tfOFpMF0cWtTMxtYZi2d39chWC8pkyxul3QOsIWkvwWuA37c3LDMrB14mO+Bo0yymAz8gfRDvNOAmyLiC02NyszMWkqZS2f/b0T8B3BpZ4GkM3OZmZkNAGXOLE6sUvaxBsdhZmYtrMtkIelYST8GdpE0s/C4DXiu3oYlXSZphaQFhbIOSbMlPZqfR+ZySbpI0kJJD0jaq7DOiXn5RyVVS1xmZtZktZqhfgUsA0YB3yiUr6Lc2FBTgW8D0wtlk4FbI+J8SZPz688DRwC75ce+wPeAfSV1AF8CJpKuwJonaWZE/LHE/s3MrEG6PLOIiCcj4jbgEOCOiLidlDzGUOK2qhExB3i+ovhIYFqengZ8oFA+PZJfAyMkjQYOA2ZHxPM5QcwGDi9bOTMza4wyfRZzgGGSdgRuJd0lb2o397d9RCwDyM+dP+7bEVhcWG5JLuuq3MzMelGZZKGIeAX4IPCtiDgKGNfgOKqdqUSN8o03IJ0q6V5J965hdUODMzMb6EolC0n7A8cBP8llZS65rWZ5bl4iP6/I5UtI98noNAZYWqN8IxFxSURMjIiJQxjazfDMzKyaMsniTOBs4IaIeEjSrsAvurm/may/FPdE4MZC+Qn5qqj9gBdzM9Us4FBJI/OVU4fmMjMz60VlhiifQ+q36Hy9CPhUvfUkXQUcBIyStIR0VdP5wLWSTgGeAv4uL34TMAlYCLxC6hchIp6X9GXgnrzcuRFR2WluZmZNpnxr7X5la3XEvjq4r8MwM2srt8SMeRExsdq8Ms1QZmY2wJW5+dEBZcrMzKz/KnNm8a2SZWZWgu8BYe2oyw7ufLnsXwPbSvpsYdbWwKBmB2bWX/keENaOal0NtTkwPC+zVaF8JXB0M4MyM7PW0mWyyGNB3S5pakQ82YsxmZlZiynzS+ypkja6vjYi3t2EeMzMrAWVSRafK0wPAz4ErG1OOGZm1orK/IJ7XkXRLyXd3qR4zMysBZX5nUVH4TFK0mHAG3ohNrM+4UtbzTZWphlqHuuHC18LPA6c0sygzHrTrKXzN7ic1Ze2mm2sTDPULr0RiFlfcXIwq69MM9QwSZ+V9CNJ10v6jKRhvRGcWU+5ScmsMco0Q00HVrF+iI9jgctZP7y4WcvyWYNZY5RJFm+JiD0Kr38h6f5mBWRmZq2nzECCv8l3rwNA0r7AL5sXkpmZtZoyZxb7km55+lR+/UbgYUkPAhERuzctOjMzawllksXhTY/CzMxaWplmqK9ExJPFR7Gs2QHawOSrmMxaS5lkMb74QtJgYO/mhGOW+Coms9bSZbKQdLakVcDuklZKWpVfLwdu7MlOJT0h6UFJ8yXdm8s6JM2W9Gh+HpnLJekiSQslPSBpr57s21qDzxzM2kuXySIizouIrYCvR8TWEbFVfrw+Is5uwL7/JiImRMTE/HoycGtE7Abcml8DHAHslh+nAt9rwL6tj/nMway9lOngvlnSgZWFETGnwbEcCRyUp6cBtwGfz+XTIyKAX0saIWl0RCxr8P7NzKwLZZLFWYXpYcA+pMEFe3LzowB+lm+q9P2IuATYvjMBRMQySdvlZXcEFhfWXZLLNkgWkk4lnXkwjNf1IDQzM6tUZiDB9xVfS9oJuKCH+z0gIpbmhDBb0u9qLKtqYW1UkBLOJQBbq2Oj+WZm1n1lroaqtAR4W092GhFL8/MK4AbS2cpySaMB8vOKwv52Kqw+Bljak/2bmdmmqXtmIelbrP8mvxkwAej22FCStgQ2i4hVefpQ4FxgJnAicH5+7rziaiZwhqSrSb8mf9H9FWZmvatMn8W9hem1wFUR0ZOxobYHbpDUuf8fRsRPJd0DXCvpFOAp1o9qexMwCVgIvAKc1IN9m5lZN5RJFtcAbyadXTwWEX/uyQ4jYhGwR5Xy54CDq5QHcHpP9mlmZj1T60d5gyVdQOozmAZcASyWdIGkIb0VoJmZ9b1aHdxfBzqAXSJi74jYE3gTMAL4994IzszMWkOtZPFe4OMRsaqzICJWAv9I6kMwM7MBolayiNxfUFm4jiq/czAzs/6rVrL4raQTKgslHQ/U+hGdmZn1M7Wuhjod+JGkk0nDewTwdmAL4KheiM3MzFpEl8kiIp4G9pX0btI9LQTcHBG39lZwZmbWGsqMDfVz4Oe9EIsVzFo638N4m1nL6M7YUNYLnCjMrJU4WZiZWV1OFmZmVpeThZmZ1eVkYWZmdTlZmJlZXU4WZmZWl5OFmZnV5WRhZmZ1OVmYmVldThZmZlZX2yQLSYdLekTSQkmTG7ntWUvnN3JzZmb9TlskC0mDgO8ARwDjgGMljWvU9j0Ok5lZbW2RLIB9gIURsSgiXgWuBo7s45jMzAaMdkkWOwKLC6+X5DIzM+sFde9n0SJUpWyD+4BLOhU4Nb9cfUvMWND0qJpvFPBsXwfRQ65Da3AdWkOr12Hnrma0S7JYAuxUeD0GWFpcICIuAS4BkHRvREzsvfCaoz/Uw3VoDa5Da2jnOrRLM9Q9wG6SdpG0OXAMMLOPYzIzGzDa4swiItZKOgOYBQwCLouIh/o4LDOzAaMtkgVARNwE3FRy8UuaGUsv6g/1cB1ag+vQGtq2DoqI+kuZmdmA1i59FmZm1of6XbJo5rAgjSDpCUkPSpov6d5c1iFptqRH8/PIXC5JF+W6PCBpr8J2TszLPyrpxCbHfJmkFZIWFMoaFrOkvfN7sjCvW+1S6WbUYYqkp/OxmC9pUmHe2TmeRyQdViiv+veVL764O9ftmnwhRqPrsJOkX0h6WNJDks7M5W1zLGrUoW2OhaRhkuZKuj/X4V9r7VfS0Px6YZ4/trt161MR0W8epM7vx4Bdgc2B+4FxfR1XRYxPAKMqyi4AJufpycDX8vQk4GbS70z2A+7O5R3Aovw8Mk+PbGLMBwJ7AQuaETMwF9g/r3MzcEQv1WEK8Lkqy47LfztDgV3y39SgWn9fwLXAMXn6YuAfm1CH0cBeeXor4Pc51rY5FjXq0DbHIr83w/P0EODu/P5W3S/wSeDiPH0McE1369aXj/52ZtGuw4IcCUzL09OADxTKp0fya2CEpNHAYcDsiHg+Iv4IzAYOb1ZwETEHeL4ZMed5W0fEXZH+g6YXttXsOnTlSODqiFgdEY8DC0l/W1X/vvK373cDM/L6xfejYSJiWUTcl6dXAQ+TRjJom2NRow5dabljkd/Pl/LLIfkRNfZbPD4zgINznJtUt0bWoTv6W7Joh2FBAviZpHlKvzoH2D4ilkH6ZwK2y+Vd1acV6tmomHfM05XlveWM3ERzWWfzDZteh9cDL0TE2orypslNGXuSvtW25bGoqAO00bGQNEjSfGAFKdk+VmO/f4k1z38xx9nK/98b6W/Jou6wIC3ggIjYizSC7umSDqyxbFf1aeV6bmrMfVmX7wFvAiYAy4Bv5PKWroOk4cD1wKcjYmWtRauUtUQ9qtShrY5FRKyLiAmk0ST2Ad5aY78tWYdN1d+SRd1hQfpaRCzNzyuAG0h/aMtzEwD5eUVevKv6tEI9GxXzkjxdWd50EbE8/9O/BlxKOhbUibVa+bOkJp7BFeUNJ2kI6UP2yoj4US5uq2NRrQ7teCxy3C8At5H6LLra719izfO3ITWJtvL/98b6utOkkQ/SjwwXkTqLOjuGxvd1XIX4tgS2Kkz/itTX8HU27KC8IE+/hw07KOfm8g7gcVLn5Mg83dHk2MeyYedww2ImDeeyH+s7VSf1Uh1GF6Y/Q2o/BhjPhh2Pi0idjl3+fQHXsWHn5iebEL9I/QjfrChvm2NRow5tcyyAbYEReXoL4A7gvV3tFzidDTu4r+1u3fry0ac7b0qF0hUgvye1IX6hr+OpiG3XfODvBx7qjI/Ufnkr8Gh+7vzHFemmT48BDwITC9s6mdQhthA4qclxX0VqGlhD+tZzSiNjBiYCC/I63yb/WLQX6nB5jvEB0lhjxQ+sL+R4HqFwRVBXf1/52M7NdbsOGNqEOryD1BzxADA/Pya107GoUYe2ORbA7sBvcqwLgH+ptV9gWH69MM/ftbt168uHf8FtZmZ19bc+CzMzawInCzMzq8vJwszM6nKyMDOzupwszMysLicLM0DSS/WX2uRtjpX00U1c55xGx2HWCE4WZs0zFtikZAE4WVhLcrIwK5B0kKTbJM2Q9DtJV3be00HpXiRfy/cymCvpzbl8qqSjC9voPEs5H3hnvj/DZyr2M1rSnDxvgaR3Sjof2CKXXZmXOz7va76k70sa1LkPSd+QdJ+kWyVtm8s/Jem3eUC+q5v+htmA4WRhtrE9gU+T7jewK3BAYd7KiNiH9Ovmb9bZzmTgjoiYEBEXVsz7KDAr0mB0ewDzI2Iy8Ke8/HGS3gp8hDT45ARgHXBcXn9L4L5Ig1LeDnypsM89I2J34BObXHOzLjhZmG1sbkQsiTSo3XxSc1KnqwrP+/dgH/cAJ0maAvxVpHs7VDoY2Bu4Jw+HfTApeQG8BlyTp68gDaMBaQiKKyUdD6zFrEGcLMw2trowvY40sFunqDK9lvy/lJus6t7GM9LNmA4EngYul3RClcUETMtnGhMi4i0RMaWrTebn95DGg9obmFcYBdWsR5wszDbNRwrPd+XpJ0gfzpDuaDYkT68i3Tp0I5J2BlZExKXAD0i3fAVYk4fwhjQo4NGStsvrdOT1IP3vdvaTfBS4U9JmwE4R8Qvgn4ARwPBu1tNsA/7WYbZphkq6m/RhfWwuuxS4UdJc0gf8y7n8AWCtpPuBqRX9FgcBZ0laA7wEdJ5ZXAI8IOm+3G/xRdKdFTcjjZh7OvBk3sd4SfNId177CGl46yskbUM6K7kw0v0WzHrMo86alSTpCdIw38+2QCwvRYTPGqzXuBnKzMzq8pmFmZnV5TMLMzOry8nCzMzqcrIwM7O6nCzMzKwuJwszM6vLycLMzOr6X9UlOMHeUp3dAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test abc-3\n",
    "\n",
    "# probs_seq1 = [[\n",
    "#             0.06390443, 0.42124858, 0.27323887, 0.06870235, 0.0361254,\n",
    "#             0.18184413, 0.16493624\n",
    "#         ], [\n",
    "#             0.03309247, 0.42866108, 0.24390638, 0.09699597, 0.31895462,\n",
    "#             0.0094893, 0.06890021\n",
    "#         ], [\n",
    "#             0.218104, 0.49992557, 0.18245131, 0.08503348, 0.14903535,\n",
    "#             0.08424043, 0.08120984\n",
    "#         ], [\n",
    "#             0.12094152, 0.41162472, 0.01473646, 0.28045061, 0.24246305,\n",
    "#             0.05206269, 0.09772094\n",
    "#         ], [\n",
    "#             0.9333387, 0.40550838, 0.00301669, 0.21745861, 0.20803985,\n",
    "#             0.41317442, 0.01946335\n",
    "#         ], [\n",
    "#             0.16468227, 0.4180699, 0.1906545, 0.53963251, 0.19860937,\n",
    "#             0.04377724, 0.01457421\n",
    "#         ]]\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "torch.set_printoptions(precision=2)\n",
    "\n",
    "\n",
    "import ctc\n",
    "\n",
    "T, B, C = length_sec*100, 1, 27\n",
    "t = lenght_char\n",
    "blank = 0\n",
    "device = 'cpu'\n",
    "seed = 1\n",
    "atol = 1e-3\n",
    "for set_seed in [torch.manual_seed] + ([torch.cuda.manual_seed_all] if device == 'cuda' else []):\n",
    "\tset_seed(seed)\n",
    "tictoc = lambda: (device == 'cuda' and torch.cuda.synchronize()) or time.time()\n",
    "\n",
    "logits = torch.randn(T, B, C, device = device).requires_grad_()\n",
    "# import numpy\n",
    "# logits = torch.reshape(torch.as_tensor(numpy.array(numpy.log(probs_seq1)),device = device), (6,1,7)).requires_grad_()\n",
    "\n",
    "targets = torch.randint(blank + 1, C, (B, t), dtype = torch.long, device = device)\n",
    "# targets = torch.reshape(torch.as_tensor(numpy.array([1,2,3]), dtype = torch.long, device = device), (B, t))\n",
    "\n",
    "\n",
    "\n",
    "input_lengths = torch.full((B,), T, dtype = torch.long, device = device)\n",
    "target_lengths = torch.full((B,), t, dtype = torch.long, device = device)\n",
    "log_probs = logits.log_softmax(dim = -1)\n",
    "# print(\"log_probs\",log_probs)\n",
    "# print('Device:', device)\n",
    "# print('Log-probs shape (time X batch X channels):', 'x'.join(map(str, log_probs.shape)))\n",
    "\n",
    "# tic = tictoc()\n",
    "# builtin_ctc = F.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank = 0, reduction = 'none')\n",
    "# toc = tictoc()\n",
    "# builtin_ctc_grad, = torch.autograd.grad(builtin_ctc.sum(), logits, retain_graph = True)\n",
    "# print('Built-in CTC loss', 'fwd', toc - tic, 'bwd', tictoc() - toc)\n",
    "\n",
    "# tic = tictoc()\n",
    "# custom_ctc = ctc_loss(log_probs, targets, input_lengths, target_lengths, blank = 0, reduction = 'none')\n",
    "# toc = tictoc()\n",
    "# custom_ctc_grad, = torch.autograd.grad(custom_ctc.sum(), logits, retain_graph = True)\n",
    "# print('Custom CTC loss', 'fwd', toc - tic, 'bwd', tictoc() - toc)\n",
    "\n",
    "# ce_alignment_targets = ctc_alignment_targets(log_probs, targets, input_lengths, target_lengths, blank = 0)\n",
    "# ce_ctc = -ce_alignment_targets * log_probs\n",
    "# ce_ctc_grad, = torch.autograd.grad(ce_ctc.sum(), logits, retain_graph = True)\n",
    "\n",
    "# print('Custom loss matches:', torch.allclose(builtin_ctc, custom_ctc, atol = atol))\n",
    "# print('Grad matches:', torch.allclose(builtin_ctc_grad, custom_ctc_grad, atol = atol))\n",
    "# print('CE grad matches:', torch.allclose(builtin_ctc_grad, ce_ctc_grad, atol = atol))\n",
    "\n",
    "tic = tictoc()\n",
    "alignment = ctc.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank = 0, reduction = 'none', alignment = True)\n",
    "print('Alignment: ',tictoc() - tic)\n",
    "\n",
    "a = alignment[:, 0, :target_lengths[0]]\n",
    "# print(a.t().cpu())\n",
    "plt.subplot(211)\n",
    "plt.title('Input-Output Viterbi alignment')\n",
    "plt.imshow(a.t().cpu(), origin = 'lower', aspect = 'auto')\n",
    "plt.xlabel('Input steps')\n",
    "plt.ylabel('Output steps')\n",
    "# plt.subplot(212)\n",
    "# plt.title('CTC alignment targets')\n",
    "# a = ce_alignment_targets[:, 0, :]\n",
    "# plt.imshow(a.t().cpu(), origin = 'lower', aspect = 'auto')\n",
    "# plt.xlabel('Input steps')\n",
    "# plt.ylabel(f'Output symbols, blank {blank}')\n",
    "# plt.subplots_adjust(hspace = 0.5)\n",
    "# plt.savefig('alignment.png')\n",
    "# torch.set_printoptions(precision=2)\n",
    "# print(a.t().cpu())\n",
    "# print(targets[:,0])\n",
    "# print(logits)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment:  38.76677966117859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Output steps')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test abc-3\n",
    "\n",
    "# probs_seq1 = [[\n",
    "#             0.06390443, 0.42124858, 0.27323887, 0.06870235, 0.0361254,\n",
    "#             0.18184413, 0.16493624\n",
    "#         ], [\n",
    "#             0.03309247, 0.42866108, 0.24390638, 0.09699597, 0.31895462,\n",
    "#             0.0094893, 0.06890021\n",
    "#         ], [\n",
    "#             0.218104, 0.49992557, 0.18245131, 0.08503348, 0.14903535,\n",
    "#             0.08424043, 0.08120984\n",
    "#         ], [\n",
    "#             0.12094152, 0.41162472, 0.01473646, 0.28045061, 0.24246305,\n",
    "#             0.05206269, 0.09772094\n",
    "#         ], [\n",
    "#             0.9333387, 0.40550838, 0.00301669, 0.21745861, 0.20803985,\n",
    "#             0.41317442, 0.01946335\n",
    "#         ], [\n",
    "#             0.16468227, 0.4180699, 0.1906545, 0.53963251, 0.19860937,\n",
    "#             0.04377724, 0.01457421\n",
    "#         ]]\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "torch.set_printoptions(precision=2)\n",
    "\n",
    "\n",
    "import ctc\n",
    "\n",
    "T, B, C = length_sec*200, 1, 27\n",
    "t = lenght_char\n",
    "blank = 0\n",
    "device = 'cpu'\n",
    "seed = 1\n",
    "atol = 1e-3\n",
    "for set_seed in [torch.manual_seed] + ([torch.cuda.manual_seed_all] if device == 'cuda' else []):\n",
    "\tset_seed(seed)\n",
    "tictoc = lambda: (device == 'cuda' and torch.cuda.synchronize()) or time.time()\n",
    "\n",
    "logits = torch.randn(T, B, C, device = device).requires_grad_()\n",
    "# import numpy\n",
    "# logits = torch.reshape(torch.as_tensor(numpy.array(numpy.log(probs_seq1)),device = device), (6,1,7)).requires_grad_()\n",
    "\n",
    "targets = torch.randint(blank + 1, C, (B, t), dtype = torch.long, device = device)\n",
    "# targets = torch.reshape(torch.as_tensor(numpy.array([1,2,3]), dtype = torch.long, device = device), (B, t))\n",
    "\n",
    "\n",
    "\n",
    "input_lengths = torch.full((B,), T, dtype = torch.long, device = device)\n",
    "target_lengths = torch.full((B,), t, dtype = torch.long, device = device)\n",
    "log_probs = logits.log_softmax(dim = -1)\n",
    "# print(\"log_probs\",log_probs)\n",
    "# print('Device:', device)\n",
    "# print('Log-probs shape (time X batch X channels):', 'x'.join(map(str, log_probs.shape)))\n",
    "\n",
    "# tic = tictoc()\n",
    "# builtin_ctc = F.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank = 0, reduction = 'none')\n",
    "# toc = tictoc()\n",
    "# builtin_ctc_grad, = torch.autograd.grad(builtin_ctc.sum(), logits, retain_graph = True)\n",
    "# print('Built-in CTC loss', 'fwd', toc - tic, 'bwd', tictoc() - toc)\n",
    "\n",
    "# tic = tictoc()\n",
    "# custom_ctc = ctc_loss(log_probs, targets, input_lengths, target_lengths, blank = 0, reduction = 'none')\n",
    "# toc = tictoc()\n",
    "# custom_ctc_grad, = torch.autograd.grad(custom_ctc.sum(), logits, retain_graph = True)\n",
    "# print('Custom CTC loss', 'fwd', toc - tic, 'bwd', tictoc() - toc)\n",
    "\n",
    "# ce_alignment_targets = ctc_alignment_targets(log_probs, targets, input_lengths, target_lengths, blank = 0)\n",
    "# ce_ctc = -ce_alignment_targets * log_probs\n",
    "# ce_ctc_grad, = torch.autograd.grad(ce_ctc.sum(), logits, retain_graph = True)\n",
    "\n",
    "# print('Custom loss matches:', torch.allclose(builtin_ctc, custom_ctc, atol = atol))\n",
    "# print('Grad matches:', torch.allclose(builtin_ctc_grad, custom_ctc_grad, atol = atol))\n",
    "# print('CE grad matches:', torch.allclose(builtin_ctc_grad, ce_ctc_grad, atol = atol))\n",
    "\n",
    "tic = tictoc()\n",
    "alignment = ctc.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank = 0, reduction = 'none', alignment = True)\n",
    "print('Alignment: ',tictoc() - tic)\n",
    "\n",
    "a = alignment[:, 0, :target_lengths[0]]\n",
    "# print(a.t().cpu())\n",
    "plt.subplot(211)\n",
    "plt.title('Input-Output Viterbi alignment')\n",
    "plt.imshow(a.t().cpu(), origin = 'lower', aspect = 'auto')\n",
    "plt.xlabel('Input steps')\n",
    "plt.ylabel('Output steps')\n",
    "# plt.subplot(212)\n",
    "# plt.title('CTC alignment targets')\n",
    "# a = ce_alignment_targets[:, 0, :]\n",
    "# plt.imshow(a.t().cpu(), origin = 'lower', aspect = 'auto')\n",
    "# plt.xlabel('Input steps')\n",
    "# plt.ylabel(f'Output symbols, blank {blank}')\n",
    "# plt.subplots_adjust(hspace = 0.5)\n",
    "# plt.savefig('alignment.png')\n",
    "# torch.set_printoptions(precision=2)\n",
    "# print(a.t().cpu())\n",
    "# print(targets[:,0])\n",
    "# print(logits)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.3656 mb\n",
      "system usage:  7.872\n"
     ]
    }
   ],
   "source": [
    "print(T*B*C*4/1000000,\"mb\")\n",
    "print(\"system usage: \", 64*0.123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
